Attaching to broker, debezium, debezium-ui, postgres_master, postgres_slave, rest-proxy, schema-registry, zookeeper
zookeeper        | ===> User
zookeeper        | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
zookeeper        | ===> Configuring ...
postgres_slave   | 
postgres_slave   | PostgreSQL Database directory appears to contain a database; Skipping initialization
postgres_slave   | 
zookeeper        | ===> Running preflight checks ... 
zookeeper        | ===> Check if /var/lib/zookeeper/data is writable ...
zookeeper        | ===> Check if /var/lib/zookeeper/log is writable ...
zookeeper        | ===> Launching ... 
zookeeper        | ===> Launching zookeeper ... 
zookeeper        | [2025-01-14 19:31:21,116] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,119] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,119] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,119] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,119] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,120] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper        | [2025-01-14 19:31:21,120] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper        | [2025-01-14 19:31:21,120] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper        | [2025-01-14 19:31:21,120] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
zookeeper        | [2025-01-14 19:31:21,121] INFO Log4j 1.2 jmx support not found; jmx disabled. (org.apache.zookeeper.jmx.ManagedUtil)
zookeeper        | [2025-01-14 19:31:21,121] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,121] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,121] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,121] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,122] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper        | [2025-01-14 19:31:21,122] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
zookeeper        | [2025-01-14 19:31:21,127] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@5f683daf (org.apache.zookeeper.server.ServerMetrics)
zookeeper        | [2025-01-14 19:31:21,128] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper        | [2025-01-14 19:31:21,134] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,134] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:host.name=zookeeper (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.version=11.0.17 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.4.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.4.2.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/connect-api-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.4.jar:/usr/bin/../share/java/kafka/commons-lang3-3.12.0.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/connect-transforms-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.3.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/jline-3.21.0.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.79.Final.jar:/usr/bin/../share/java/kafka/reload4j-1.2.19.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.4.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.6.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.79.Final.jar:/usr/bin/../share/java/kafka/kafka-shell-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/connect-mirror-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.4.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jose4j-0.7.9.jar:/usr/bin/../share/java/kafka/netty-common-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/connect-runtime-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/kafka-tools-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.2-1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.79.Final.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.4.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.79.Final.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/kafka-storage-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-raft-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.4.jar:/usr/bin/../share/java/kafka/kafka-clients-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-json-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/trogdor-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.79.Final.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.48.v20220622.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:os.version=5.15.167.4-microsoft-standard-WSL2 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:os.memory.free=491MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,135] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,136] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,136] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)
zookeeper        | [2025-01-14 19:31:21,137] INFO minSessionTimeout set to 4000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,137] INFO maxSessionTimeout set to 40000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,137] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper        | [2025-01-14 19:31:21,138] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper        | [2025-01-14 19:31:21,138] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper        | [2025-01-14 19:31:21,138] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper        | [2025-01-14 19:31:21,138] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper        | [2025-01-14 19:31:21,138] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper        | [2025-01-14 19:31:21,138] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper        | [2025-01-14 19:31:21,138] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper        | [2025-01-14 19:31:21,139] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,139] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,139] INFO Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,150] INFO Logging initialized @258ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
zookeeper        | [2025-01-14 19:31:21,192] WARN o.e.j.s.ServletContextHandler@7eecb5b8{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper        | [2025-01-14 19:31:21,192] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper        | [2025-01-14 19:31:21,201] INFO jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.17+8-LTS (org.eclipse.jetty.server.Server)
zookeeper        | [2025-01-14 19:31:21,214] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
zookeeper        | [2025-01-14 19:31:21,214] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
zookeeper        | [2025-01-14 19:31:21,215] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
zookeeper        | [2025-01-14 19:31:21,216] WARN ServletContext@o.e.j.s.ServletContextHandler@7eecb5b8{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)
zookeeper        | [2025-01-14 19:31:21,221] INFO Started o.e.j.s.ServletContextHandler@7eecb5b8{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper        | [2025-01-14 19:31:21,230] INFO Started ServerConnector@5b247367{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
zookeeper        | [2025-01-14 19:31:21,230] INFO Started @338ms (org.eclipse.jetty.server.Server)
zookeeper        | [2025-01-14 19:31:21,230] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
zookeeper        | [2025-01-14 19:31:21,233] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper        | [2025-01-14 19:31:21,233] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper        | [2025-01-14 19:31:21,234] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 3 selector thread(s), 40 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper        | [2025-01-14 19:31:21,234] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper        | [2025-01-14 19:31:21,241] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper        | [2025-01-14 19:31:21,242] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper        | [2025-01-14 19:31:21,242] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
zookeeper        | [2025-01-14 19:31:21,242] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
zookeeper        | [2025-01-14 19:31:21,245] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
zookeeper        | [2025-01-14 19:31:21,245] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper        | [2025-01-14 19:31:21,246] INFO Snapshot loaded in 4 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)
zookeeper        | [2025-01-14 19:31:21,246] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper        | [2025-01-14 19:31:21,246] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper        | [2025-01-14 19:31:21,251] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
zookeeper        | [2025-01-14 19:31:21,251] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
zookeeper        | [2025-01-14 19:31:21,259] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
zookeeper        | [2025-01-14 19:31:21,259] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)
zookeeper        | [2025-01-14 19:31:24,497] INFO The list of known four letter word commands is : [{1936881266=srvr, 1937006964=stat, 2003003491=wchc, 1685417328=dump, 1668445044=crst, 1936880500=srst, 1701738089=envi, 1668247142=conf, -720899=telnet close, 1751217000=hash, 2003003507=wchs, 2003003504=wchp, 1684632179=dirs, 1668247155=cons, 1835955314=mntr, 1769173615=isro, 1920298859=ruok, 1735683435=gtmk, 1937010027=stmk}] (org.apache.zookeeper.server.command.FourLetterCommands)
zookeeper        | [2025-01-14 19:31:24,497] INFO The list of enabled four letter word commands is : [[srvr]] (org.apache.zookeeper.server.command.FourLetterCommands)
zookeeper        | [2025-01-14 19:31:24,497] INFO Processing srvr command from /172.18.0.3:50732 (org.apache.zookeeper.server.NIOServerCnxn)
broker           | ===> User
broker           | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
broker           | ===> Configuring ...
broker           | ===> Running preflight checks ... 
broker           | ===> Check if /var/lib/kafka/data is writable ...
broker           | ===> Check if Zookeeper is healthy ...
broker           | [2025-01-14 19:31:27,148] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:host.name=broker (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.version=11.0.17 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.class.path=/usr/share/java/cp-base-new/kafka-storage-api-7.3.1-ccs.jar:/usr/share/java/cp-base-new/minimal-json-0.9.5.jar:/usr/share/java/cp-base-new/jackson-databind-2.13.4.2.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.36.jar:/usr/share/java/cp-base-new/jmx_prometheus_javaagent-0.17.2.jar:/usr/share/java/cp-base-new/jackson-core-2.13.4.jar:/usr/share/java/cp-base-new/kafka-metadata-7.3.1-ccs.jar:/usr/share/java/cp-base-new/slf4j-reload4j-1.7.36.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/json-simple-1.1.1.jar:/usr/share/java/cp-base-new/jolokia-core-1.7.1.jar:/usr/share/java/cp-base-new/reload4j-1.2.19.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.13.4.jar:/usr/share/java/cp-base-new/scala-library-2.13.10.jar:/usr/share/java/cp-base-new/kafka_2.13-7.3.1-ccs.jar:/usr/share/java/cp-base-new/gson-2.9.0.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.6.0.jar:/usr/share/java/cp-base-new/disk-usage-agent-7.3.1.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.4.jar:/usr/share/java/cp-base-new/snakeyaml-1.32.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.13.4.jar:/usr/share/java/cp-base-new/utility-belt-7.3.1.jar:/usr/share/java/cp-base-new/jose4j-0.7.9.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.10.jar:/usr/share/java/cp-base-new/logredactor-metrics-1.0.10.jar:/usr/share/java/cp-base-new/snappy-java-1.1.8.4.jar:/usr/share/java/cp-base-new/zstd-jni-1.5.2-1.jar:/usr/share/java/cp-base-new/jolokia-jvm-1.7.1.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.13.4.jar:/usr/share/java/cp-base-new/zookeeper-3.6.3.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-1.0.2.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.13.4.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.6.3.jar:/usr/share/java/cp-base-new/common-utils-7.3.1.jar:/usr/share/java/cp-base-new/kafka-storage-7.3.1-ccs.jar:/usr/share/java/cp-base-new/kafka-raft-7.3.1-ccs.jar:/usr/share/java/cp-base-new/logredactor-1.0.10.jar:/usr/share/java/cp-base-new/kafka-server-common-7.3.1-ccs.jar:/usr/share/java/cp-base-new/metrics-core-4.1.12.1.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/re2j-1.6.jar:/usr/share/java/cp-base-new/kafka-clients-7.3.1-ccs.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/lz4-java-1.8.0.jar:/usr/share/java/cp-base-new/jackson-annotations-2.13.4.jar (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:os.version=5.15.167.4-microsoft-standard-WSL2 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:os.memory.free=239MB (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:os.memory.max=3970MB (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,148] INFO Client environment:os.memory.total=250MB (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,150] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@516be40f (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,151] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
broker           | [2025-01-14 19:31:27,154] INFO jute.maxbuffer value is 1048575 Bytes (org.apache.zookeeper.ClientCnxnSocket)
broker           | [2025-01-14 19:31:27,157] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,166] INFO Opening socket connection to server zookeeper/172.18.0.3:2181. (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,167] INFO SASL config status: Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,171] INFO Socket connection established, initiating session, client: /172.18.0.5:60706, server: zookeeper/172.18.0.3:2181 (org.apache.zookeeper.ClientCnxn)
zookeeper        | [2025-01-14 19:31:27,176] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
broker           | [2025-01-14 19:31:27,185] INFO Session establishment complete on server zookeeper/172.18.0.3:2181, session id = 0x1000826c84a0000, negotiated timeout = 40000 (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,299] INFO Session: 0x1000826c84a0000 closed (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,299] INFO EventThread shut down for session: 0x1000826c84a0000 (org.apache.zookeeper.ClientCnxn)
broker           | Using log4j config /etc/kafka/log4j.properties
broker           | ===> Launching ... 
broker           | ===> Launching kafka ... 
broker           | [2025-01-14 19:31:27,630] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
broker           | [2025-01-14 19:31:27,806] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
broker           | [2025-01-14 19:31:27,843] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
broker           | [2025-01-14 19:31:27,843] INFO starting (kafka.server.KafkaServer)
broker           | [2025-01-14 19:31:27,844] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
broker           | [2025-01-14 19:31:27,851] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:host.name=broker (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.version=11.0.17 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.13.4.jar:/usr/bin/../share/java/kafka/jackson-databind-2.13.4.2.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.36.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/connect-api-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jackson-core-2.13.4.jar:/usr/bin/../share/java/kafka/commons-lang3-3.12.0.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/connect-transforms-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-reload4j-1.7.36.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.13.3.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/jline-3.21.0.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.79.Final.jar:/usr/bin/../share/java/kafka/reload4j-1.2.19.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.13.4.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/scala-library-2.13.10.jar:/usr/bin/../share/java/kafka/plexus-utils-3.3.0.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.6.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.13.4.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.79.Final.jar:/usr/bin/../share/java/kafka/kafka-shell-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.4.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.2.jar:/usr/bin/../share/java/kafka/connect-mirror-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.13.4.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jose4j-0.7.9.jar:/usr/bin/../share/java/kafka/netty-common-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/connect-runtime-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.10.jar:/usr/bin/../share/java/kafka/kafka-tools-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.79.Final.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.2-1.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.79.Final.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.13.4.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.48.v20220622.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.2.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.79.Final.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.4.jar:/usr/bin/../share/java/kafka/kafka-storage-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-raft-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/swagger-annotations-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.13.4.jar:/usr/bin/../share/java/kafka/kafka-clients-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-json-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/trogdor-7.3.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.79.Final.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/rocksdbjni-7.1.2.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.48.v20220622.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:os.version=5.15.167.4-microsoft-standard-WSL2 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:os.memory.free=1008MB (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,853] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,854] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@628c4ac0 (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:31:27,857] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
broker           | [2025-01-14 19:31:27,860] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,861] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
broker           | [2025-01-14 19:31:27,863] INFO Opening socket connection to server zookeeper/172.18.0.3:2181. (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,866] INFO Socket connection established, initiating session, client: /172.18.0.5:60718, server: zookeeper/172.18.0.3:2181 (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,871] INFO Session establishment complete on server zookeeper/172.18.0.3:2181, session id = 0x1000826c84a0001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:31:27,873] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
broker           | [2025-01-14 19:31:28,009] INFO Cluster ID = __R4m4AETriw5M5Mmqky6Q (kafka.server.KafkaServer)
broker           | [2025-01-14 19:31:28,011] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
broker           | [2025-01-14 19:31:28,034] INFO KafkaConfig values: 
broker           | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
broker           | 	alter.config.policy.class.name = null
broker           | 	alter.log.dirs.replication.quota.window.num = 11
broker           | 	alter.log.dirs.replication.quota.window.size.seconds = 1
broker           | 	authorizer.class.name = 
broker           | 	auto.create.topics.enable = true
broker           | 	auto.leader.rebalance.enable = true
broker           | 	background.threads = 10
broker           | 	broker.heartbeat.interval.ms = 2000
broker           | 	broker.id = 1
broker           | 	broker.id.generation.enable = true
broker           | 	broker.rack = null
broker           | 	broker.session.timeout.ms = 9000
broker           | 	client.quota.callback.class = null
broker           | 	compression.type = producer
broker           | 	connection.failed.authentication.delay.ms = 100
broker           | 	connections.max.idle.ms = 600000
broker           | 	connections.max.reauth.ms = 0
broker           | 	control.plane.listener.name = null
broker           | 	controlled.shutdown.enable = true
broker           | 	controlled.shutdown.max.retries = 3
broker           | 	controlled.shutdown.retry.backoff.ms = 5000
broker           | 	controller.listener.names = null
broker           | 	controller.quorum.append.linger.ms = 25
broker           | 	controller.quorum.election.backoff.max.ms = 1000
broker           | 	controller.quorum.election.timeout.ms = 1000
broker           | 	controller.quorum.fetch.timeout.ms = 2000
broker           | 	controller.quorum.request.timeout.ms = 2000
broker           | 	controller.quorum.retry.backoff.ms = 20
broker           | 	controller.quorum.voters = []
broker           | 	controller.quota.window.num = 11
broker           | 	controller.quota.window.size.seconds = 1
broker           | 	controller.socket.timeout.ms = 30000
broker           | 	create.topic.policy.class.name = null
broker           | 	default.replication.factor = 1
broker           | 	delegation.token.expiry.check.interval.ms = 3600000
broker           | 	delegation.token.expiry.time.ms = 86400000
broker           | 	delegation.token.master.key = null
broker           | 	delegation.token.max.lifetime.ms = 604800000
broker           | 	delegation.token.secret.key = null
broker           | 	delete.records.purgatory.purge.interval.requests = 1
broker           | 	delete.topic.enable = true
broker           | 	early.start.listeners = null
broker           | 	fetch.max.bytes = 57671680
broker           | 	fetch.purgatory.purge.interval.requests = 1000
broker           | 	group.initial.rebalance.delay.ms = 0
broker           | 	group.max.session.timeout.ms = 1800000
broker           | 	group.max.size = 2147483647
broker           | 	group.min.session.timeout.ms = 6000
broker           | 	initial.broker.registration.timeout.ms = 60000
broker           | 	inter.broker.listener.name = null
broker           | 	inter.broker.protocol.version = 3.3-IV3
broker           | 	kafka.metrics.polling.interval.secs = 10
broker           | 	kafka.metrics.reporters = []
broker           | 	leader.imbalance.check.interval.seconds = 300
broker           | 	leader.imbalance.per.broker.percentage = 10
broker           | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
broker           | 	listeners = PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
broker           | 	log.cleaner.backoff.ms = 15000
broker           | 	log.cleaner.dedupe.buffer.size = 134217728
broker           | 	log.cleaner.delete.retention.ms = 86400000
broker           | 	log.cleaner.enable = true
broker           | 	log.cleaner.io.buffer.load.factor = 0.9
broker           | 	log.cleaner.io.buffer.size = 524288
broker           | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
broker           | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
broker           | 	log.cleaner.min.cleanable.ratio = 0.5
broker           | 	log.cleaner.min.compaction.lag.ms = 0
broker           | 	log.cleaner.threads = 1
broker           | 	log.cleanup.policy = [delete]
broker           | 	log.dir = /tmp/kafka-logs
broker           | 	log.dirs = /var/lib/kafka/data
broker           | 	log.flush.interval.messages = 9223372036854775807
broker           | 	log.flush.interval.ms = null
broker           | 	log.flush.offset.checkpoint.interval.ms = 60000
broker           | 	log.flush.scheduler.interval.ms = 9223372036854775807
broker           | 	log.flush.start.offset.checkpoint.interval.ms = 60000
broker           | 	log.index.interval.bytes = 4096
broker           | 	log.index.size.max.bytes = 10485760
broker           | 	log.message.downconversion.enable = true
broker           | 	log.message.format.version = 3.0-IV1
broker           | 	log.message.timestamp.difference.max.ms = 9223372036854775807
broker           | 	log.message.timestamp.type = CreateTime
broker           | 	log.preallocate = false
broker           | 	log.retention.bytes = -1
broker           | 	log.retention.check.interval.ms = 300000
broker           | 	log.retention.hours = 168
broker           | 	log.retention.minutes = null
broker           | 	log.retention.ms = null
broker           | 	log.roll.hours = 168
broker           | 	log.roll.jitter.hours = 0
broker           | 	log.roll.jitter.ms = null
broker           | 	log.roll.ms = null
broker           | 	log.segment.bytes = 1073741824
broker           | 	log.segment.delete.delay.ms = 60000
broker           | 	max.connection.creation.rate = 2147483647
broker           | 	max.connections = 2147483647
broker           | 	max.connections.per.ip = 2147483647
broker           | 	max.connections.per.ip.overrides = 
broker           | 	max.incremental.fetch.session.cache.slots = 1000
broker           | 	message.max.bytes = 1048588
broker           | 	metadata.log.dir = null
broker           | 	metadata.log.max.record.bytes.between.snapshots = 20971520
broker           | 	metadata.log.segment.bytes = 1073741824
broker           | 	metadata.log.segment.min.bytes = 8388608
broker           | 	metadata.log.segment.ms = 604800000
broker           | 	metadata.max.idle.interval.ms = 500
broker           | 	metadata.max.retention.bytes = -1
broker           | 	metadata.max.retention.ms = 604800000
broker           | 	metric.reporters = []
broker           | 	metrics.num.samples = 2
broker           | 	metrics.recording.level = INFO
broker           | 	metrics.sample.window.ms = 30000
broker           | 	min.insync.replicas = 1
broker           | 	node.id = 1
broker           | 	num.io.threads = 8
broker           | 	num.network.threads = 3
broker           | 	num.partitions = 1
broker           | 	num.recovery.threads.per.data.dir = 1
broker           | 	num.replica.alter.log.dirs.threads = null
broker           | 	num.replica.fetchers = 1
broker           | 	offset.metadata.max.bytes = 4096
broker           | 	offsets.commit.required.acks = -1
broker           | 	offsets.commit.timeout.ms = 5000
broker           | 	offsets.load.buffer.size = 5242880
broker           | 	offsets.retention.check.interval.ms = 600000
broker           | 	offsets.retention.minutes = 10080
broker           | 	offsets.topic.compression.codec = 0
broker           | 	offsets.topic.num.partitions = 50
broker           | 	offsets.topic.replication.factor = 1
broker           | 	offsets.topic.segment.bytes = 104857600
broker           | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
broker           | 	password.encoder.iterations = 4096
broker           | 	password.encoder.key.length = 128
broker           | 	password.encoder.keyfactory.algorithm = null
broker           | 	password.encoder.old.secret = null
broker           | 	password.encoder.secret = null
broker           | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
broker           | 	process.roles = []
broker           | 	producer.purgatory.purge.interval.requests = 1000
broker           | 	queued.max.request.bytes = -1
broker           | 	queued.max.requests = 500
broker           | 	quota.window.num = 11
broker           | 	quota.window.size.seconds = 1
broker           | 	remote.log.index.file.cache.total.size.bytes = 1073741824
broker           | 	remote.log.manager.task.interval.ms = 30000
broker           | 	remote.log.manager.task.retry.backoff.max.ms = 30000
broker           | 	remote.log.manager.task.retry.backoff.ms = 500
broker           | 	remote.log.manager.task.retry.jitter = 0.2
broker           | 	remote.log.manager.thread.pool.size = 10
broker           | 	remote.log.metadata.manager.class.name = null
broker           | 	remote.log.metadata.manager.class.path = null
broker           | 	remote.log.metadata.manager.impl.prefix = null
broker           | 	remote.log.metadata.manager.listener.name = null
broker           | 	remote.log.reader.max.pending.tasks = 100
broker           | 	remote.log.reader.threads = 10
broker           | 	remote.log.storage.manager.class.name = null
broker           | 	remote.log.storage.manager.class.path = null
broker           | 	remote.log.storage.manager.impl.prefix = null
broker           | 	remote.log.storage.system.enable = false
broker           | 	replica.fetch.backoff.ms = 1000
broker           | 	replica.fetch.max.bytes = 1048576
broker           | 	replica.fetch.min.bytes = 1
broker           | 	replica.fetch.response.max.bytes = 10485760
broker           | 	replica.fetch.wait.max.ms = 500
broker           | 	replica.high.watermark.checkpoint.interval.ms = 5000
broker           | 	replica.lag.time.max.ms = 30000
broker           | 	replica.selector.class = null
broker           | 	replica.socket.receive.buffer.bytes = 65536
broker           | 	replica.socket.timeout.ms = 30000
broker           | 	replication.quota.window.num = 11
broker           | 	replication.quota.window.size.seconds = 1
broker           | 	request.timeout.ms = 30000
broker           | 	reserved.broker.max.id = 1000
broker           | 	sasl.client.callback.handler.class = null
broker           | 	sasl.enabled.mechanisms = [GSSAPI]
broker           | 	sasl.jaas.config = null
broker           | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
broker           | 	sasl.kerberos.min.time.before.relogin = 60000
broker           | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
broker           | 	sasl.kerberos.service.name = null
broker           | 	sasl.kerberos.ticket.renew.jitter = 0.05
broker           | 	sasl.kerberos.ticket.renew.window.factor = 0.8
broker           | 	sasl.login.callback.handler.class = null
broker           | 	sasl.login.class = null
broker           | 	sasl.login.connect.timeout.ms = null
broker           | 	sasl.login.read.timeout.ms = null
broker           | 	sasl.login.refresh.buffer.seconds = 300
broker           | 	sasl.login.refresh.min.period.seconds = 60
broker           | 	sasl.login.refresh.window.factor = 0.8
broker           | 	sasl.login.refresh.window.jitter = 0.05
broker           | 	sasl.login.retry.backoff.max.ms = 10000
broker           | 	sasl.login.retry.backoff.ms = 100
broker           | 	sasl.mechanism.controller.protocol = GSSAPI
broker           | 	sasl.mechanism.inter.broker.protocol = GSSAPI
broker           | 	sasl.oauthbearer.clock.skew.seconds = 30
broker           | 	sasl.oauthbearer.expected.audience = null
broker           | 	sasl.oauthbearer.expected.issuer = null
broker           | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
broker           | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
broker           | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
broker           | 	sasl.oauthbearer.jwks.endpoint.url = null
broker           | 	sasl.oauthbearer.scope.claim.name = scope
broker           | 	sasl.oauthbearer.sub.claim.name = sub
broker           | 	sasl.oauthbearer.token.endpoint.url = null
broker           | 	sasl.server.callback.handler.class = null
broker           | 	sasl.server.max.receive.size = 524288
broker           | 	security.inter.broker.protocol = PLAINTEXT
broker           | 	security.providers = null
broker           | 	socket.connection.setup.timeout.max.ms = 30000
broker           | 	socket.connection.setup.timeout.ms = 10000
broker           | 	socket.listen.backlog.size = 50
broker           | 	socket.receive.buffer.bytes = 102400
broker           | 	socket.request.max.bytes = 104857600
broker           | 	socket.send.buffer.bytes = 102400
broker           | 	ssl.cipher.suites = []
broker           | 	ssl.client.auth = none
broker           | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
broker           | 	ssl.endpoint.identification.algorithm = https
broker           | 	ssl.engine.factory.class = null
broker           | 	ssl.key.password = null
broker           | 	ssl.keymanager.algorithm = SunX509
broker           | 	ssl.keystore.certificate.chain = null
broker           | 	ssl.keystore.key = null
broker           | 	ssl.keystore.location = null
broker           | 	ssl.keystore.password = null
broker           | 	ssl.keystore.type = JKS
broker           | 	ssl.principal.mapping.rules = DEFAULT
broker           | 	ssl.protocol = TLSv1.3
broker           | 	ssl.provider = null
broker           | 	ssl.secure.random.implementation = null
broker           | 	ssl.trustmanager.algorithm = PKIX
broker           | 	ssl.truststore.certificates = null
broker           | 	ssl.truststore.location = null
broker           | 	ssl.truststore.password = null
broker           | 	ssl.truststore.type = JKS
broker           | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
broker           | 	transaction.max.timeout.ms = 900000
broker           | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
broker           | 	transaction.state.log.load.buffer.size = 5242880
broker           | 	transaction.state.log.min.isr = 1
broker           | 	transaction.state.log.num.partitions = 50
broker           | 	transaction.state.log.replication.factor = 1
broker           | 	transaction.state.log.segment.bytes = 104857600
broker           | 	transactional.id.expiration.ms = 604800000
broker           | 	unclean.leader.election.enable = false
broker           | 	zookeeper.clientCnxnSocket = null
broker           | 	zookeeper.connect = zookeeper:2181
broker           | 	zookeeper.connection.timeout.ms = null
broker           | 	zookeeper.max.in.flight.requests = 10
broker           | 	zookeeper.session.timeout.ms = 18000
broker           | 	zookeeper.set.acl = false
broker           | 	zookeeper.ssl.cipher.suites = null
broker           | 	zookeeper.ssl.client.enable = false
broker           | 	zookeeper.ssl.crl.enable = false
broker           | 	zookeeper.ssl.enabled.protocols = null
broker           | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
broker           | 	zookeeper.ssl.keystore.location = null
broker           | 	zookeeper.ssl.keystore.password = null
broker           | 	zookeeper.ssl.keystore.type = null
broker           | 	zookeeper.ssl.ocsp.enable = false
broker           | 	zookeeper.ssl.protocol = TLSv1.2
broker           | 	zookeeper.ssl.truststore.location = null
broker           | 	zookeeper.ssl.truststore.password = null
broker           | 	zookeeper.ssl.truststore.type = null
broker           |  (kafka.server.KafkaConfig)
broker           | [2025-01-14 19:31:28,055] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:31:28,055] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:31:28,056] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:31:28,058] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:31:28,074] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
broker           | [2025-01-14 19:31:28,076] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
broker           | [2025-01-14 19:31:28,081] INFO Loaded 0 logs in 6ms. (kafka.log.LogManager)
broker           | [2025-01-14 19:31:28,081] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
broker           | [2025-01-14 19:31:28,083] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
broker           | [2025-01-14 19:31:28,089] INFO Starting the log cleaner (kafka.log.LogCleaner)
broker           | [2025-01-14 19:31:28,122] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
broker           | [2025-01-14 19:31:28,130] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
broker           | [2025-01-14 19:31:28,136] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
broker           | [2025-01-14 19:31:28,151] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:31:28,337] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker           | [2025-01-14 19:31:28,339] INFO Awaiting socket connections on 0.0.0.0:29092. (kafka.network.DataPlaneAcceptor)
broker           | [2025-01-14 19:31:28,352] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
broker           | [2025-01-14 19:31:28,353] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
broker           | [2025-01-14 19:31:28,353] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
broker           | [2025-01-14 19:31:28,355] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
broker           | [2025-01-14 19:31:28,359] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Starting (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:31:28,370] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,371] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,371] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,372] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,379] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
broker           | [2025-01-14 19:31:28,390] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
broker           | [2025-01-14 19:31:28,400] INFO Stat of the created znode at /brokers/ids/1 is: 27,27,1736883088396,1736883088396,1,0,0,72066556700000257,263,0,27
broker           |  (kafka.zk.KafkaZkClient)
broker           | [2025-01-14 19:31:28,401] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092, czxid (broker epoch): 27 (kafka.zk.KafkaZkClient)
broker           | [2025-01-14 19:31:28,428] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
broker           | [2025-01-14 19:31:28,433] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,436] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,437] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,438] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
broker           | [2025-01-14 19:31:28,446] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:28,464] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,465] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:28,467] INFO [Controller id=1] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map()) (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,469] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
broker           | [2025-01-14 19:31:28,474] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
broker           | [2025-01-14 19:31:28,477] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker           | [2025-01-14 19:31:28,477] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
broker           | [2025-01-14 19:31:28,483] INFO [MetadataCache brokerId=1] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)
broker           | [2025-01-14 19:31:28,483] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,485] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,487] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,488] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,495] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 27) (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,497] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:31:28,498] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,501] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
broker           | [2025-01-14 19:31:28,505] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
broker           | [2025-01-14 19:31:28,505] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,505] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,505] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,506] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,507] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,507] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,508] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,508] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
broker           | [2025-01-14 19:31:28,508] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
broker           | [2025-01-14 19:31:28,508] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,510] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:28,513] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
broker           | [2025-01-14 19:31:28,513] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
broker           | [2025-01-14 19:31:28,516] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
broker           | [2025-01-14 19:31:28,516] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
broker           | [2025-01-14 19:31:28,516] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
broker           | [2025-01-14 19:31:28,516] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
broker           | [2025-01-14 19:31:28,517] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
broker           | [2025-01-14 19:31:28,518] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,518] INFO [RequestSendThread controllerId=1] Controller 1 connected to broker:29092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
broker           | [2025-01-14 19:31:28,520] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
broker           | [2025-01-14 19:31:28,522] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,523] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,524] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,524] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,525] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,527] INFO Kafka version: 7.3.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2025-01-14 19:31:28,527] INFO Kafka commitId: 8628b0341c3c46766f141043367cc0052f75b090 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2025-01-14 19:31:28,527] INFO Kafka startTimeMs: 1736883088523 (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2025-01-14 19:31:28,531] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
broker           | [2025-01-14 19:31:28,538] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:28,559] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:28,561] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use node broker:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:31:28,562] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Recorded new controller, from now on will use node broker:29092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
schema-registry  | ===> User
schema-registry  | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
schema-registry  | ===> Configuring ...
rest-proxy       | ===> User
rest-proxy       | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
rest-proxy       | ===> Configuring ...
debezium         | Using BOOTSTRAP_SERVERS=broker:29092
debezium         | Plugins are loaded from /kafka/connect
debezium         | Debezium Scripting enabled!
debezium         | Using the following environment variables:
debezium         |       GROUP_ID=1
debezium         |       CONFIG_STORAGE_TOPIC=connect_configs
debezium         |       OFFSET_STORAGE_TOPIC=connect_offsets
debezium         |       STATUS_STORAGE_TOPIC=connect_statuses
debezium         |       BOOTSTRAP_SERVERS=broker:29092
debezium         |       REST_HOST_NAME=172.18.0.8
debezium         |       REST_PORT=8083
debezium         |       ADVERTISED_HOST_NAME=172.18.0.8
debezium         |       ADVERTISED_PORT=8083
debezium         |       KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
debezium         |       VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
debezium         |       OFFSET_FLUSH_INTERVAL_MS=60000
debezium         |       OFFSET_FLUSH_TIMEOUT_MS=5000
debezium         |       SHUTDOWN_TIMEOUT=10000
debezium         | --- Setting property from CONNECT_REST_ADVERTISED_PORT: rest.advertised.port=8083
debezium         | --- Setting property from CONNECT_OFFSET_STORAGE_TOPIC: offset.storage.topic=connect_offsets
debezium         | --- Setting property from CONNECT_KEY_CONVERTER: key.converter=org.apache.kafka.connect.json.JsonConverter
debezium         | --- Setting property from CONNECT_CONFIG_STORAGE_TOPIC: config.storage.topic=connect_configs
debezium         | --- Setting property from CONNECT_GROUP_ID: group.id=1
debezium         | --- Setting property from CONNECT_REST_ADVERTISED_HOST_NAME: rest.advertised.host.name=172.18.0.8
debezium         | --- Setting property from CONNECT_REST_HOST_NAME: rest.host.name=172.18.0.8
debezium         | --- Setting property from CONNECT_VALUE_CONVERTER: value.converter=org.apache.kafka.connect.json.JsonConverter
debezium         | --- Setting property from CONNECT_REST_PORT: rest.port=8083
debezium         | --- Setting property from CONNECT_STATUS_STORAGE_TOPIC: status.storage.topic=connect_statuses
debezium         | --- Setting property from CONNECT_OFFSET_FLUSH_TIMEOUT_MS: offset.flush.timeout.ms=5000
debezium         | --- Setting property from CONNECT_PLUGIN_PATH: plugin.path=/kafka/connect
debezium         | --- Setting property from CONNECT_OFFSET_FLUSH_INTERVAL_MS: offset.flush.interval.ms=60000
debezium         | --- Setting property from CONNECT_BOOTSTRAP_SERVERS: bootstrap.servers=broker:29092
debezium         | --- Setting property from CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS: task.shutdown.graceful.timeout.ms=10000
debezium         | 2025-01-14 19:31:31,756 INFO   ||  Kafka Connect worker initializing ...   [org.apache.kafka.connect.cli.AbstractConnectCli]
debezium         | 2025-01-14 19:31:31,759 INFO   ||  WorkerInfo values: 
debezium         | 	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/kafka/logs, -Dlog4j.configuration=file:/kafka/config/log4j.properties
debezium         | 	jvm.spec = Red Hat, Inc., OpenJDK 64-Bit Server VM, 21.0.5, 21.0.5+11
debezium         | 	jvm.classpath = /kafka/libs/activation-1.1.1.jar:/kafka/libs/aopalliance-repackaged-2.6.1.jar:/kafka/libs/argparse4j-0.7.0.jar:/kafka/libs/audience-annotations-0.12.0.jar:/kafka/libs/caffeine-2.9.3.jar:/kafka/libs/checker-qual-3.19.0.jar:/kafka/libs/commons-beanutils-1.9.4.jar:/kafka/libs/commons-cli-1.4.jar:/kafka/libs/commons-collections-3.2.2.jar:/kafka/libs/commons-digester-2.1.jar:/kafka/libs/commons-io-2.11.0.jar:/kafka/libs/commons-lang3-3.8.1.jar:/kafka/libs/commons-logging-1.2.jar:/kafka/libs/commons-validator-1.7.jar:/kafka/libs/connect-api-3.7.0.jar:/kafka/libs/connect-basic-auth-extension-3.7.0.jar:/kafka/libs/connect-json-3.7.0.jar:/kafka/libs/connect-mirror-3.7.0.jar:/kafka/libs/connect-mirror-client-3.7.0.jar:/kafka/libs/connect-runtime-3.7.0.jar:/kafka/libs/connect-transforms-3.7.0.jar:/kafka/libs/error_prone_annotations-2.10.0.jar:/kafka/libs/hk2-api-2.6.1.jar:/kafka/libs/hk2-locator-2.6.1.jar:/kafka/libs/hk2-utils-2.6.1.jar:/kafka/libs/jackson-annotations-2.16.0.jar:/kafka/libs/jackson-core-2.16.0.jar:/kafka/libs/jackson-databind-2.16.0.jar:/kafka/libs/jackson-dataformat-csv-2.16.0.jar:/kafka/libs/jackson-datatype-jdk8-2.16.0.jar:/kafka/libs/jackson-jaxrs-base-2.16.0.jar:/kafka/libs/jackson-jaxrs-json-provider-2.16.0.jar:/kafka/libs/jackson-module-jaxb-annotations-2.16.0.jar:/kafka/libs/jackson-module-scala_2.13-2.16.0.jar:/kafka/libs/jakarta.activation-api-1.2.2.jar:/kafka/libs/jakarta.annotation-api-1.3.5.jar:/kafka/libs/jakarta.inject-2.6.1.jar:/kafka/libs/jakarta.validation-api-2.0.2.jar:/kafka/libs/jakarta.ws.rs-api-2.1.6.jar:/kafka/libs/jakarta.xml.bind-api-2.3.3.jar:/kafka/libs/javassist-3.29.2-GA.jar:/kafka/libs/javax.activation-api-1.2.0.jar:/kafka/libs/javax.annotation-api-1.3.2.jar:/kafka/libs/javax.servlet-api-3.1.0.jar:/kafka/libs/javax.ws.rs-api-2.1.1.jar:/kafka/libs/jaxb-api-2.3.1.jar:/kafka/libs/jersey-client-2.39.1.jar:/kafka/libs/jersey-common-2.39.1.jar:/kafka/libs/jersey-container-servlet-2.39.1.jar:/kafka/libs/jersey-container-servlet-core-2.39.1.jar:/kafka/libs/jersey-hk2-2.39.1.jar:/kafka/libs/jersey-server-2.39.1.jar:/kafka/libs/jetty-client-9.4.53.v20231009.jar:/kafka/libs/jetty-continuation-9.4.53.v20231009.jar:/kafka/libs/jetty-http-9.4.53.v20231009.jar:/kafka/libs/jetty-io-9.4.53.v20231009.jar:/kafka/libs/jetty-security-9.4.53.v20231009.jar:/kafka/libs/jetty-server-9.4.53.v20231009.jar:/kafka/libs/jetty-servlet-9.4.53.v20231009.jar:/kafka/libs/jetty-servlets-9.4.53.v20231009.jar:/kafka/libs/jetty-util-9.4.53.v20231009.jar:/kafka/libs/jetty-util-ajax-9.4.53.v20231009.jar:/kafka/libs/jline-3.22.0.jar:/kafka/libs/jolokia-jvm-1.7.2.jar:/kafka/libs/jopt-simple-5.0.4.jar:/kafka/libs/jose4j-0.9.4.jar:/kafka/libs/jsr305-3.0.2.jar:/kafka/libs/kafka-clients-3.7.0.jar:/kafka/libs/kafka-group-coordinator-3.7.0.jar:/kafka/libs/kafka-log4j-appender-3.7.0.jar:/kafka/libs/kafka-metadata-3.7.0.jar:/kafka/libs/kafka-raft-3.7.0.jar:/kafka/libs/kafka-server-3.7.0.jar:/kafka/libs/kafka-server-common-3.7.0.jar:/kafka/libs/kafka-shell-3.7.0.jar:/kafka/libs/kafka-storage-3.7.0.jar:/kafka/libs/kafka-storage-api-3.7.0.jar:/kafka/libs/kafka-streams-3.7.0.jar:/kafka/libs/kafka-streams-examples-3.7.0.jar:/kafka/libs/kafka-streams-scala_2.13-3.7.0.jar:/kafka/libs/kafka-streams-test-utils-3.7.0.jar:/kafka/libs/kafka-tools-3.7.0.jar:/kafka/libs/kafka-tools-api-3.7.0.jar:/kafka/libs/kafka_2.13-3.7.0.jar:/kafka/libs/lz4-java-1.8.0.jar:/kafka/libs/maven-artifact-3.8.8.jar:/kafka/libs/metrics-core-2.2.0.jar:/kafka/libs/metrics-core-4.1.12.1.jar:/kafka/libs/netty-buffer-4.1.100.Final.jar:/kafka/libs/netty-codec-4.1.100.Final.jar:/kafka/libs/netty-common-4.1.100.Final.jar:/kafka/libs/netty-handler-4.1.100.Final.jar:/kafka/libs/netty-resolver-4.1.100.Final.jar:/kafka/libs/netty-transport-4.1.100.Final.jar:/kafka/libs/netty-transport-classes-epoll-4.1.100.Final.jar:/kafka/libs/netty-transport-native-epoll-4.1.100.Final.jar:/kafka/libs/netty-transport-native-unix-common-4.1.100.Final.jar:/kafka/libs/opentelemetry-proto-1.0.0-alpha.jar:/kafka/libs/osgi-resource-locator-1.0.3.jar:/kafka/libs/paranamer-2.8.jar:/kafka/libs/pcollections-4.0.1.jar:/kafka/libs/plexus-utils-3.3.1.jar:/kafka/libs/protobuf-java-3.23.4.jar:/kafka/libs/reflections-0.10.2.jar:/kafka/libs/reload4j-1.2.25.jar:/kafka/libs/rocksdbjni-7.9.2.jar:/kafka/libs/scala-collection-compat_2.13-2.10.0.jar:/kafka/libs/scala-java8-compat_2.13-1.0.2.jar:/kafka/libs/scala-library-2.13.12.jar:/kafka/libs/scala-logging_2.13-3.9.4.jar:/kafka/libs/scala-reflect-2.13.12.jar:/kafka/libs/slf4j-api-1.7.36.jar:/kafka/libs/slf4j-reload4j-1.7.36.jar:/kafka/libs/snappy-java-1.1.10.5.jar:/kafka/libs/swagger-annotations-2.2.8.jar:/kafka/libs/trogdor-3.7.0.jar:/kafka/libs/zookeeper-3.8.3.jar:/kafka/libs/zookeeper-jute-3.8.3.jar:/kafka/libs/zstd-jni-1.5.5-6.jar
debezium         | 	os.spec = Linux, amd64, 5.15.167.4-microsoft-standard-WSL2
debezium         | 	os.vcpus = 20
debezium         |    [org.apache.kafka.connect.runtime.WorkerInfo]
debezium         | 2025-01-14 19:31:31,760 INFO   ||  Scanning for plugin classes. This might take a moment ...   [org.apache.kafka.connect.cli.AbstractConnectCli]
debezium         | 2025-01-14 19:31:31,784 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-informix   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:31,812 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:31,909 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-informix/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:31,910 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-jdbc   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:31,920 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:31,940 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-jdbc/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,019 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mysql   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,027 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,052 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mysql/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,054 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-spanner   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,066 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,083 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-spanner/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,083 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-oracle   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,185 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,202 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-oracle/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,217 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-postgres   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,221 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,238 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-postgres/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,239 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mongodb   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,244 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,267 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mongodb/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,267 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-ibmi   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
rest-proxy       | ===> Running preflight checks ... 
rest-proxy       | ===> Check if Kafka is healthy ...
debezium         | 2025-01-14 19:31:32,275 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,289 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-ibmi/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,294 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-sqlserver   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,298 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,319 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-sqlserver/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,332 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-vitess   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,339 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,355 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-vitess/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,356 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-db2   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,361 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,376 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-db2/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,377 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mariadb   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,384 INFO   ||  Using up-to-date JsonConverter implementation   [io.debezium.converters.CloudEventsConverter]
debezium         | 2025-01-14 19:31:32,403 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mariadb/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,404 INFO   ||  Loading plugin from: classpath   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
schema-registry  | ===> Running preflight checks ... 
debezium         | 2025-01-14 19:31:32,408 INFO   ||  Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,409 INFO   ||  Scanning plugins with ServiceLoaderScanner took 625 ms   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
schema-registry  | ===> Check if Kafka is healthy ...
debezium         | 2025-01-14 19:31:32,410 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-informix   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,540 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-informix/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,540 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-jdbc   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
rest-proxy       | [2025-01-14 19:31:32,768] INFO AdminClientConfig values: 
rest-proxy       | 	bootstrap.servers = [broker:29092]
rest-proxy       | 	client.dns.lookup = use_all_dns_ips
rest-proxy       | 	client.id = 
rest-proxy       | 	connections.max.idle.ms = 300000
rest-proxy       | 	default.api.timeout.ms = 60000
rest-proxy       | 	metadata.max.age.ms = 300000
rest-proxy       | 	metric.reporters = []
rest-proxy       | 	metrics.num.samples = 2
rest-proxy       | 	metrics.recording.level = INFO
rest-proxy       | 	metrics.sample.window.ms = 30000
rest-proxy       | 	receive.buffer.bytes = 65536
rest-proxy       | 	reconnect.backoff.max.ms = 1000
rest-proxy       | 	reconnect.backoff.ms = 50
rest-proxy       | 	request.timeout.ms = 30000
rest-proxy       | 	retries = 2147483647
rest-proxy       | 	retry.backoff.ms = 100
rest-proxy       | 	sasl.client.callback.handler.class = null
rest-proxy       | 	sasl.jaas.config = null
rest-proxy       | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
rest-proxy       | 	sasl.kerberos.min.time.before.relogin = 60000
rest-proxy       | 	sasl.kerberos.service.name = null
rest-proxy       | 	sasl.kerberos.ticket.renew.jitter = 0.05
rest-proxy       | 	sasl.kerberos.ticket.renew.window.factor = 0.8
rest-proxy       | 	sasl.login.callback.handler.class = null
rest-proxy       | 	sasl.login.class = null
rest-proxy       | 	sasl.login.connect.timeout.ms = null
rest-proxy       | 	sasl.login.read.timeout.ms = null
rest-proxy       | 	sasl.login.refresh.buffer.seconds = 300
rest-proxy       | 	sasl.login.refresh.min.period.seconds = 60
rest-proxy       | 	sasl.login.refresh.window.factor = 0.8
rest-proxy       | 	sasl.login.refresh.window.jitter = 0.05
rest-proxy       | 	sasl.login.retry.backoff.max.ms = 10000
rest-proxy       | 	sasl.login.retry.backoff.ms = 100
rest-proxy       | 	sasl.mechanism = GSSAPI
rest-proxy       | 	sasl.oauthbearer.clock.skew.seconds = 30
rest-proxy       | 	sasl.oauthbearer.expected.audience = null
rest-proxy       | 	sasl.oauthbearer.expected.issuer = null
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
rest-proxy       | 	sasl.oauthbearer.jwks.endpoint.url = null
rest-proxy       | 	sasl.oauthbearer.scope.claim.name = scope
rest-proxy       | 	sasl.oauthbearer.sub.claim.name = sub
rest-proxy       | 	sasl.oauthbearer.token.endpoint.url = null
rest-proxy       | 	security.protocol = PLAINTEXT
rest-proxy       | 	security.providers = null
rest-proxy       | 	send.buffer.bytes = 131072
rest-proxy       | 	socket.connection.setup.timeout.max.ms = 30000
rest-proxy       | 	socket.connection.setup.timeout.ms = 10000
rest-proxy       | 	ssl.cipher.suites = null
rest-proxy       | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
rest-proxy       | 	ssl.endpoint.identification.algorithm = https
rest-proxy       | 	ssl.engine.factory.class = null
rest-proxy       | 	ssl.key.password = null
rest-proxy       | 	ssl.keymanager.algorithm = SunX509
rest-proxy       | 	ssl.keystore.certificate.chain = null
rest-proxy       | 	ssl.keystore.key = null
rest-proxy       | 	ssl.keystore.location = null
rest-proxy       | 	ssl.keystore.password = null
rest-proxy       | 	ssl.keystore.type = JKS
rest-proxy       | 	ssl.protocol = TLSv1.3
rest-proxy       | 	ssl.provider = null
rest-proxy       | 	ssl.secure.random.implementation = null
rest-proxy       | 	ssl.trustmanager.algorithm = PKIX
rest-proxy       | 	ssl.truststore.certificates = null
rest-proxy       | 	ssl.truststore.location = null
rest-proxy       | 	ssl.truststore.password = null
rest-proxy       | 	ssl.truststore.type = JKS
rest-proxy       |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry  | [2025-01-14 19:31:32,862] INFO AdminClientConfig values: 
schema-registry  | 	bootstrap.servers = [broker:29092]
schema-registry  | 	client.dns.lookup = use_all_dns_ips
schema-registry  | 	client.id = 
schema-registry  | 	connections.max.idle.ms = 300000
schema-registry  | 	default.api.timeout.ms = 60000
schema-registry  | 	metadata.max.age.ms = 300000
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.recording.level = INFO
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	receive.buffer.bytes = 65536
schema-registry  | 	reconnect.backoff.max.ms = 1000
schema-registry  | 	reconnect.backoff.ms = 50
schema-registry  | 	request.timeout.ms = 30000
schema-registry  | 	retries = 2147483647
schema-registry  | 	retry.backoff.ms = 100
schema-registry  | 	sasl.client.callback.handler.class = null
schema-registry  | 	sasl.jaas.config = null
schema-registry  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	sasl.kerberos.service.name = null
schema-registry  | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	sasl.login.callback.handler.class = null
schema-registry  | 	sasl.login.class = null
schema-registry  | 	sasl.login.connect.timeout.ms = null
schema-registry  | 	sasl.login.read.timeout.ms = null
schema-registry  | 	sasl.login.refresh.buffer.seconds = 300
schema-registry  | 	sasl.login.refresh.min.period.seconds = 60
schema-registry  | 	sasl.login.refresh.window.factor = 0.8
schema-registry  | 	sasl.login.refresh.window.jitter = 0.05
schema-registry  | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.login.retry.backoff.ms = 100
schema-registry  | 	sasl.mechanism = GSSAPI
schema-registry  | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry  | 	sasl.oauthbearer.expected.audience = null
schema-registry  | 	sasl.oauthbearer.expected.issuer = null
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry  | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry  | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry  | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry  | 	security.protocol = PLAINTEXT
schema-registry  | 	security.providers = null
schema-registry  | 	send.buffer.bytes = 131072
schema-registry  | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry  | 	socket.connection.setup.timeout.ms = 10000
schema-registry  | 	ssl.cipher.suites = null
schema-registry  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry  | 	ssl.endpoint.identification.algorithm = https
schema-registry  | 	ssl.engine.factory.class = null
schema-registry  | 	ssl.key.password = null
schema-registry  | 	ssl.keymanager.algorithm = SunX509
schema-registry  | 	ssl.keystore.certificate.chain = null
schema-registry  | 	ssl.keystore.key = null
schema-registry  | 	ssl.keystore.location = null
schema-registry  | 	ssl.keystore.password = null
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.protocol = TLSv1.3
schema-registry  | 	ssl.provider = null
schema-registry  | 	ssl.secure.random.implementation = null
schema-registry  | 	ssl.trustmanager.algorithm = PKIX
schema-registry  | 	ssl.truststore.certificates = null
schema-registry  | 	ssl.truststore.location = null
schema-registry  | 	ssl.truststore.password = null
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  |  (org.apache.kafka.clients.admin.AdminClientConfig)
rest-proxy       | [2025-01-14 19:31:32,884] INFO Kafka version: 7.3.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy       | [2025-01-14 19:31:32,884] INFO Kafka commitId: 8628b0341c3c4676 (org.apache.kafka.common.utils.AppInfoParser)
rest-proxy       | [2025-01-14 19:31:32,884] INFO Kafka startTimeMs: 1736883092883 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:32,976] INFO Kafka version: 7.3.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:32,976] INFO Kafka commitId: 8628b0341c3c4676 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:32,976] INFO Kafka startTimeMs: 1736883092974 (org.apache.kafka.common.utils.AppInfoParser)
debezium         | 2025-01-14 19:31:33,133 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-jdbc/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,138 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mysql   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,237 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mysql/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,238 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-spanner   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
rest-proxy       | Using log4j config /etc/kafka-rest/log4j.properties
rest-proxy       | ===> Launching ... 
rest-proxy       | ===> Launching kafka-rest ... 
schema-registry  | Using log4j config /etc/schema-registry/log4j.properties
schema-registry  | ===> Launching ... 
schema-registry  | ===> Launching schema-registry ... 
broker           | [2025-01-14 19:31:33,027] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:33,028] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
debezium         | 2025-01-14 19:31:32,604 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-spanner/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,605 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-oracle   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
rest-proxy       | [2025-01-14 19:31:32,762] INFO KafkaRestConfig values: 
rest-proxy       | 	access.control.allow.headers = 
rest-proxy       | 	access.control.allow.methods = 
rest-proxy       | 	access.control.allow.origin = 
rest-proxy       | 	access.control.skip.options = true
rest-proxy       | 	advertised.listeners = []
rest-proxy       | 	api.endpoints.allowlist = []
rest-proxy       | 	api.endpoints.blocklist = []
rest-proxy       | 	api.v2.enable = true
rest-proxy       | 	api.v3.enable = true
rest-proxy       | 	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
rest-proxy       | 	api.v3.produce.rate.limit.enabled = false
rest-proxy       | 	api.v3.produce.rate.limit.max.bytes.global.per.sec = 10000000
rest-proxy       | 	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
rest-proxy       | 	api.v3.produce.rate.limit.max.requests.global.per.sec = 10000
rest-proxy       | 	api.v3.produce.rate.limit.max.requests.per.sec = 10000
rest-proxy       | 	api.v3.produce.response.thread.pool.size = 20
rest-proxy       | 	authentication.method = NONE
rest-proxy       | 	authentication.realm = 
rest-proxy       | 	authentication.roles = [*]
rest-proxy       | 	authentication.skip.paths = []
rest-proxy       | 	bootstrap.servers = broker:29092
rest-proxy       | 	client.init.timeout.ms = 60000
rest-proxy       | 	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
rest-proxy       | 	client.sasl.kerberos.min.time.before.relogin = 60000
rest-proxy       | 	client.sasl.kerberos.service.name = 
rest-proxy       | 	client.sasl.kerberos.ticket.renew.jitter = 0.05
rest-proxy       | 	client.sasl.kerberos.ticket.renew.window.factor = 0.8
rest-proxy       | 	client.sasl.mechanism = GSSAPI
rest-proxy       | 	client.security.protocol = PLAINTEXT
rest-proxy       | 	client.ssl.cipher.suites = 
rest-proxy       | 	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
rest-proxy       | 	client.ssl.endpoint.identification.algorithm = 
rest-proxy       | 	client.ssl.key.password = [hidden]
rest-proxy       | 	client.ssl.keymanager.algorithm = SunX509
rest-proxy       | 	client.ssl.keystore.location = 
rest-proxy       | 	client.ssl.keystore.password = [hidden]
rest-proxy       | 	client.ssl.keystore.type = JKS
rest-proxy       | 	client.ssl.protocol = TLS
rest-proxy       | 	client.ssl.provider = 
rest-proxy       | 	client.ssl.trustmanager.algorithm = PKIX
rest-proxy       | 	client.ssl.truststore.location = 
rest-proxy       | 	client.ssl.truststore.password = [hidden]
rest-proxy       | 	client.ssl.truststore.type = JKS
rest-proxy       | 	client.timeout.ms = 500
rest-proxy       | 	client.zk.session.timeout.ms = 30000
rest-proxy       | 	compression.enable = true
rest-proxy       | 	confluent.resource.name.authority = 
rest-proxy       | 	connector.connection.limit = 0
rest-proxy       | 	consumer.instance.timeout.ms = 300000
rest-proxy       | 	consumer.iterator.backoff.ms = 50
rest-proxy       | 	consumer.iterator.timeout.ms = 1
rest-proxy       | 	consumer.request.max.bytes = 67108864
rest-proxy       | 	consumer.request.timeout.ms = 1000
rest-proxy       | 	consumer.threads = 50
rest-proxy       | 	csrf.prevention.enable = false
rest-proxy       | 	csrf.prevention.token.endpoint = /csrf
rest-proxy       | 	csrf.prevention.token.expiration.minutes = 30
rest-proxy       | 	csrf.prevention.token.max.entries = 10000
rest-proxy       | 	debug = false
rest-proxy       | 	dos.filter.delay.ms = 100
rest-proxy       | 	dos.filter.enabled = false
rest-proxy       | 	dos.filter.insert.headers = true
rest-proxy       | 	dos.filter.ip.whitelist = []
rest-proxy       | 	dos.filter.managed.attr = false
rest-proxy       | 	dos.filter.max.idle.tracker.ms = 30000
rest-proxy       | 	dos.filter.max.requests.ms = 30000
rest-proxy       | 	dos.filter.max.requests.per.connection.per.sec = 25
rest-proxy       | 	dos.filter.max.requests.per.sec = 25
rest-proxy       | 	dos.filter.max.wait.ms = 50
rest-proxy       | 	dos.filter.throttle.ms = 30000
rest-proxy       | 	dos.filter.throttled.requests = 5
rest-proxy       | 	fetch.min.bytes = -1
rest-proxy       | 	host.name = rest-proxy
rest-proxy       | 	http2.enabled = true
rest-proxy       | 	id = 
rest-proxy       | 	idle.timeout.ms = 30000
rest-proxy       | 	kafka.rest.resource.extension.class = []
rest-proxy       | 	listener.protocol.map = []
rest-proxy       | 	listeners = [http://0.0.0.0:8082]
rest-proxy       | 	metric.reporters = []
rest-proxy       | 	metrics.jmx.prefix = kafka.rest
rest-proxy       | 	metrics.num.samples = 2
rest-proxy       | 	metrics.sample.window.ms = 30000
rest-proxy       | 	metrics.tag.map = []
rest-proxy       | 	nosniff.prevention.enable = false
rest-proxy       | 	port = 8082
rest-proxy       | 	producer.threads = 5
rest-proxy       | 	proxy.protocol.enabled = false
rest-proxy       | 	rate.limit.backend = guava
rest-proxy       | 	rate.limit.costs = 
rest-proxy       | 	rate.limit.default.cost = 1
rest-proxy       | 	rate.limit.enable = false
rest-proxy       | 	rate.limit.permits.per.sec = 50
rest-proxy       | 	rate.limit.timeout.ms = 0
rest-proxy       | 	reject.options.request = false
rest-proxy       | 	request.logger.name = io.confluent.rest-utils.requests
rest-proxy       | 	request.queue.capacity = 2147483647
rest-proxy       | 	request.queue.capacity.growby = 64
rest-proxy       | 	request.queue.capacity.init = 128
rest-proxy       | 	resource.extension.classes = []
rest-proxy       | 	response.http.headers.config = 
rest-proxy       | 	response.mediatype.default = application/json
rest-proxy       | 	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
rest-proxy       | 	rest.servlet.initializor.classes = []
rest-proxy       | 	schema.registry.url = http://localhost:8081
rest-proxy       | 	server.connection.limit = 0
rest-proxy       | 	shutdown.graceful.ms = 1000
rest-proxy       | 	simpleconsumer.pool.size.max = 25
rest-proxy       | 	simpleconsumer.pool.timeout.ms = 1000
rest-proxy       | 	ssl.cipher.suites = []
rest-proxy       | 	ssl.client.auth = false
rest-proxy       | 	ssl.client.authentication = NONE
rest-proxy       | 	ssl.enabled.protocols = []
rest-proxy       | 	ssl.endpoint.identification.algorithm = null
rest-proxy       | 	ssl.key.password = [hidden]
rest-proxy       | 	ssl.keymanager.algorithm = 
rest-proxy       | 	ssl.keystore.location = 
rest-proxy       | 	ssl.keystore.password = [hidden]
rest-proxy       | 	ssl.keystore.reload = false
rest-proxy       | 	ssl.keystore.type = JKS
rest-proxy       | 	ssl.keystore.watch.location = 
rest-proxy       | 	ssl.protocol = TLS
rest-proxy       | 	ssl.provider = 
rest-proxy       | 	ssl.trustmanager.algorithm = 
rest-proxy       | 	ssl.truststore.location = 
rest-proxy       | 	ssl.truststore.password = [hidden]
rest-proxy       | 	ssl.truststore.type = JKS
rest-proxy       | 	streaming.connection.max.duration.grace.period.ms = 500
rest-proxy       | 	streaming.connection.max.duration.ms = 86400000
rest-proxy       | 	suppress.stack.trace.response = true
rest-proxy       | 	thread.pool.max = 200
rest-proxy       | 	thread.pool.min = 8
rest-proxy       | 	websocket.path.prefix = /ws
rest-proxy       | 	websocket.servlet.initializor.classes = []
rest-proxy       | 	zookeeper.connect = 
rest-proxy       |  (io.confluent.kafkarest.KafkaRestConfig)
rest-proxy       | [2025-01-14 19:31:32,796] INFO Logging initialized @425ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
rest-proxy       | [2025-01-14 19:31:32,819] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
rest-proxy       | [2025-01-14 19:31:32,870] INFO Adding listener with HTTP/2: http://0.0.0.0:8082 (io.confluent.rest.ApplicationServer)
debezium         | 2025-01-14 19:31:32,918 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-oracle/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:32,919 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-postgres   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
schema-registry  | [2025-01-14 19:31:32,932] INFO SchemaRegistryConfig values: 
schema-registry  | 	access.control.allow.headers = 
schema-registry  | 	access.control.allow.methods = 
schema-registry  | 	access.control.allow.origin = 
schema-registry  | 	access.control.skip.options = true
schema-registry  | 	authentication.method = NONE
schema-registry  | 	authentication.realm = 
schema-registry  | 	authentication.roles = [*]
schema-registry  | 	authentication.skip.paths = []
schema-registry  | 	avro.compatibility.level = 
schema-registry  | 	compression.enable = true
schema-registry  | 	connector.connection.limit = 0
schema-registry  | 	csrf.prevention.enable = false
schema-registry  | 	csrf.prevention.token.endpoint = /csrf
schema-registry  | 	csrf.prevention.token.expiration.minutes = 30
schema-registry  | 	csrf.prevention.token.max.entries = 10000
schema-registry  | 	debug = false
schema-registry  | 	dos.filter.delay.ms = 100
schema-registry  | 	dos.filter.enabled = false
schema-registry  | 	dos.filter.insert.headers = true
schema-registry  | 	dos.filter.ip.whitelist = []
schema-registry  | 	dos.filter.managed.attr = false
schema-registry  | 	dos.filter.max.idle.tracker.ms = 30000
schema-registry  | 	dos.filter.max.requests.ms = 30000
schema-registry  | 	dos.filter.max.requests.per.connection.per.sec = 25
schema-registry  | 	dos.filter.max.requests.per.sec = 25
schema-registry  | 	dos.filter.max.wait.ms = 50
schema-registry  | 	dos.filter.throttle.ms = 30000
schema-registry  | 	dos.filter.throttled.requests = 5
schema-registry  | 	host.name = schema-registry
schema-registry  | 	http2.enabled = true
schema-registry  | 	idle.timeout.ms = 30000
schema-registry  | 	inter.instance.headers.whitelist = []
schema-registry  | 	inter.instance.protocol = http
schema-registry  | 	kafkastore.bootstrap.servers = [broker:29092]
schema-registry  | 	kafkastore.checkpoint.dir = /tmp
schema-registry  | 	kafkastore.checkpoint.version = 0
schema-registry  | 	kafkastore.connection.url = 
schema-registry  | 	kafkastore.group.id = 
schema-registry  | 	kafkastore.init.timeout.ms = 60000
schema-registry  | 	kafkastore.sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	kafkastore.sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	kafkastore.sasl.kerberos.service.name = 
schema-registry  | 	kafkastore.sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	kafkastore.sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	kafkastore.sasl.mechanism = GSSAPI
schema-registry  | 	kafkastore.security.protocol = PLAINTEXT
schema-registry  | 	kafkastore.ssl.cipher.suites = 
schema-registry  | 	kafkastore.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
schema-registry  | 	kafkastore.ssl.endpoint.identification.algorithm = 
schema-registry  | 	kafkastore.ssl.key.password = [hidden]
schema-registry  | 	kafkastore.ssl.keymanager.algorithm = SunX509
schema-registry  | 	kafkastore.ssl.keystore.location = 
schema-registry  | 	kafkastore.ssl.keystore.password = [hidden]
schema-registry  | 	kafkastore.ssl.keystore.type = JKS
schema-registry  | 	kafkastore.ssl.protocol = TLS
schema-registry  | 	kafkastore.ssl.provider = 
schema-registry  | 	kafkastore.ssl.trustmanager.algorithm = PKIX
schema-registry  | 	kafkastore.ssl.truststore.location = 
schema-registry  | 	kafkastore.ssl.truststore.password = [hidden]
schema-registry  | 	kafkastore.ssl.truststore.type = JKS
schema-registry  | 	kafkastore.timeout.ms = 500
schema-registry  | 	kafkastore.topic = _schemas
schema-registry  | 	kafkastore.topic.replication.factor = 3
schema-registry  | 	kafkastore.topic.skip.validation = false
schema-registry  | 	kafkastore.update.handlers = []
schema-registry  | 	kafkastore.write.max.retries = 5
schema-registry  | 	leader.connect.timeout.ms = 60000
schema-registry  | 	leader.eligibility = true
schema-registry  | 	leader.read.timeout.ms = 60000
schema-registry  | 	listener.protocol.map = []
schema-registry  | 	listeners = [http://0.0.0.0:8081]
schema-registry  | 	master.eligibility = null
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.jmx.prefix = kafka.schema.registry
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	metrics.tag.map = []
schema-registry  | 	mode.mutability = true
schema-registry  | 	nosniff.prevention.enable = false
schema-registry  | 	port = 8081
schema-registry  | 	proxy.protocol.enabled = false
schema-registry  | 	reject.options.request = false
schema-registry  | 	request.logger.name = io.confluent.rest-utils.requests
schema-registry  | 	request.queue.capacity = 2147483647
schema-registry  | 	request.queue.capacity.growby = 64
schema-registry  | 	request.queue.capacity.init = 128
schema-registry  | 	resource.extension.class = []
schema-registry  | 	resource.extension.classes = []
schema-registry  | 	resource.static.locations = []
schema-registry  | 	response.http.headers.config = 
schema-registry  | 	response.mediatype.default = application/vnd.schemaregistry.v1+json
schema-registry  | 	response.mediatype.preferred = [application/vnd.schemaregistry.v1+json, application/vnd.schemaregistry+json, application/json]
schema-registry  | 	rest.servlet.initializor.classes = []
schema-registry  | 	schema.cache.expiry.secs = 300
schema-registry  | 	schema.cache.size = 1000
schema-registry  | 	schema.canonicalize.on.consume = []
schema-registry  | 	schema.compatibility.level = backward
schema-registry  | 	schema.providers = []
schema-registry  | 	schema.registry.group.id = schema-registry
schema-registry  | 	schema.registry.inter.instance.protocol = 
schema-registry  | 	schema.registry.resource.extension.class = []
schema-registry  | 	server.connection.limit = 0
schema-registry  | 	shutdown.graceful.ms = 1000
schema-registry  | 	ssl.cipher.suites = []
schema-registry  | 	ssl.client.auth = false
schema-registry  | 	ssl.client.authentication = NONE
schema-registry  | 	ssl.enabled.protocols = []
schema-registry  | 	ssl.endpoint.identification.algorithm = null
schema-registry  | 	ssl.key.password = [hidden]
schema-registry  | 	ssl.keymanager.algorithm = 
schema-registry  | 	ssl.keystore.location = 
schema-registry  | 	ssl.keystore.password = [hidden]
schema-registry  | 	ssl.keystore.reload = false
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.keystore.watch.location = 
schema-registry  | 	ssl.protocol = TLS
schema-registry  | 	ssl.provider = 
schema-registry  | 	ssl.trustmanager.algorithm = 
schema-registry  | 	ssl.truststore.location = 
schema-registry  | 	ssl.truststore.password = [hidden]
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  | 	suppress.stack.trace.response = true
schema-registry  | 	thread.pool.max = 200
schema-registry  | 	thread.pool.min = 8
schema-registry  | 	websocket.path.prefix = /ws
schema-registry  | 	websocket.servlet.initializor.classes = []
schema-registry  |  (io.confluent.kafka.schemaregistry.rest.SchemaRegistryConfig)
schema-registry  | [2025-01-14 19:31:32,962] INFO Logging initialized @517ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
schema-registry  | [2025-01-14 19:31:32,982] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
debezium         | 2025-01-14 19:31:33,005 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-postgres/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,006 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mongodb   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
schema-registry  | [2025-01-14 19:31:33,035] INFO Adding listener with HTTP/2: http://0.0.0.0:8081 (io.confluent.rest.ApplicationServer)
rest-proxy       | [2025-01-14 19:31:33,052] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
rest-proxy       | [2025-01-14 19:31:33,053] INFO SchemaRegistryConfig values: 
rest-proxy       | 	auto.register.schemas = false
rest-proxy       | 	basic.auth.credentials.source = URL
rest-proxy       | 	basic.auth.user.info = [hidden]
rest-proxy       | 	bearer.auth.cache.expiry.buffer.seconds = 300
rest-proxy       | 	bearer.auth.client.id = null
rest-proxy       | 	bearer.auth.client.secret = null
rest-proxy       | 	bearer.auth.credentials.source = STATIC_TOKEN
rest-proxy       | 	bearer.auth.identity.pool.id = null
rest-proxy       | 	bearer.auth.issuer.endpoint.url = null
rest-proxy       | 	bearer.auth.logical.cluster = null
rest-proxy       | 	bearer.auth.scope = null
rest-proxy       | 	bearer.auth.scope.claim.name = scope
rest-proxy       | 	bearer.auth.sub.claim.name = sub
rest-proxy       | 	bearer.auth.token = [hidden]
rest-proxy       | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
rest-proxy       | 	id.compatibility.strict = true
rest-proxy       | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
rest-proxy       | 	latest.compatibility.strict = true
rest-proxy       | 	max.schemas.per.subject = 1000
rest-proxy       | 	normalize.schemas = false
rest-proxy       | 	proxy.host = 
rest-proxy       | 	proxy.port = -1
rest-proxy       | 	schema.format = null
rest-proxy       | 	schema.reflection = false
rest-proxy       | 	schema.registry.basic.auth.user.info = [hidden]
rest-proxy       | 	schema.registry.ssl.cipher.suites = null
rest-proxy       | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
rest-proxy       | 	schema.registry.ssl.endpoint.identification.algorithm = https
rest-proxy       | 	schema.registry.ssl.engine.factory.class = null
rest-proxy       | 	schema.registry.ssl.key.password = null
rest-proxy       | 	schema.registry.ssl.keymanager.algorithm = SunX509
rest-proxy       | 	schema.registry.ssl.keystore.certificate.chain = null
rest-proxy       | 	schema.registry.ssl.keystore.key = null
rest-proxy       | 	schema.registry.ssl.keystore.location = null
rest-proxy       | 	schema.registry.ssl.keystore.password = null
rest-proxy       | 	schema.registry.ssl.keystore.type = JKS
rest-proxy       | 	schema.registry.ssl.protocol = TLSv1.3
rest-proxy       | 	schema.registry.ssl.provider = null
rest-proxy       | 	schema.registry.ssl.secure.random.implementation = null
rest-proxy       | 	schema.registry.ssl.trustmanager.algorithm = PKIX
rest-proxy       | 	schema.registry.ssl.truststore.certificates = null
rest-proxy       | 	schema.registry.ssl.truststore.location = null
rest-proxy       | 	schema.registry.ssl.truststore.password = null
rest-proxy       | 	schema.registry.ssl.truststore.type = JKS
rest-proxy       | 	schema.registry.url = [http://localhost:8081]
rest-proxy       | 	use.latest.version = false
rest-proxy       | 	use.schema.id = -1
rest-proxy       | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
rest-proxy       |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
debezium         | 2025-01-14 19:31:33,070 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mongodb/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,071 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-ibmi   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
rest-proxy       | [2025-01-14 19:31:33,074] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
rest-proxy       | [2025-01-14 19:31:33,075] INFO Binding KafkaRestApplication to all listeners. (io.confluent.rest.Application)
rest-proxy       | [2025-01-14 19:31:33,133] INFO jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.17+8-LTS (org.eclipse.jetty.server.Server)
rest-proxy       | [2025-01-14 19:31:33,159] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
rest-proxy       | [2025-01-14 19:31:33,159] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
rest-proxy       | [2025-01-14 19:31:33,160] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
debezium         | 2025-01-14 19:31:33,161 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-ibmi/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,162 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-sqlserver   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,215 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-sqlserver/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,218 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-vitess   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
schema-registry  | [2025-01-14 19:31:33,274] INFO AdminClientConfig values: 
schema-registry  | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry  | 	client.dns.lookup = use_all_dns_ips
schema-registry  | 	client.id = 
schema-registry  | 	confluent.use.controller.listener = false
schema-registry  | 	connections.max.idle.ms = 300000
schema-registry  | 	default.api.timeout.ms = 60000
schema-registry  | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
schema-registry  | 	metadata.max.age.ms = 300000
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.recording.level = INFO
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	receive.buffer.bytes = 65536
schema-registry  | 	reconnect.backoff.max.ms = 1000
schema-registry  | 	reconnect.backoff.ms = 50
schema-registry  | 	request.timeout.ms = 30000
schema-registry  | 	retries = 2147483647
schema-registry  | 	retry.backoff.ms = 100
schema-registry  | 	sasl.client.callback.handler.class = null
schema-registry  | 	sasl.jaas.config = null
schema-registry  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	sasl.kerberos.service.name = null
schema-registry  | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	sasl.login.callback.handler.class = null
schema-registry  | 	sasl.login.class = null
schema-registry  | 	sasl.login.connect.timeout.ms = null
schema-registry  | 	sasl.login.read.timeout.ms = null
schema-registry  | 	sasl.login.refresh.buffer.seconds = 300
schema-registry  | 	sasl.login.refresh.min.period.seconds = 60
schema-registry  | 	sasl.login.refresh.window.factor = 0.8
schema-registry  | 	sasl.login.refresh.window.jitter = 0.05
schema-registry  | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.login.retry.backoff.ms = 100
schema-registry  | 	sasl.mechanism = GSSAPI
schema-registry  | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry  | 	sasl.oauthbearer.expected.audience = null
schema-registry  | 	sasl.oauthbearer.expected.issuer = null
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry  | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry  | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry  | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry  | 	security.protocol = PLAINTEXT
schema-registry  | 	security.providers = null
schema-registry  | 	send.buffer.bytes = 131072
schema-registry  | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry  | 	socket.connection.setup.timeout.ms = 10000
schema-registry  | 	ssl.cipher.suites = null
schema-registry  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry  | 	ssl.endpoint.identification.algorithm = https
schema-registry  | 	ssl.engine.factory.class = null
schema-registry  | 	ssl.key.password = null
schema-registry  | 	ssl.keymanager.algorithm = SunX509
schema-registry  | 	ssl.keystore.certificate.chain = null
schema-registry  | 	ssl.keystore.key = null
schema-registry  | 	ssl.keystore.location = null
schema-registry  | 	ssl.keystore.password = null
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.protocol = TLSv1.3
schema-registry  | 	ssl.provider = null
schema-registry  | 	ssl.secure.random.implementation = null
schema-registry  | 	ssl.trustmanager.algorithm = PKIX
schema-registry  | 	ssl.truststore.certificates = null
schema-registry  | 	ssl.truststore.location = null
schema-registry  | 	ssl.truststore.password = null
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  |  (org.apache.kafka.clients.admin.AdminClientConfig)
rest-proxy       | [2025-01-14 19:31:33,302] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
rest-proxy       | [2025-01-14 19:31:33,303] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
rest-proxy       | [2025-01-14 19:31:33,308] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
rest-proxy       | [2025-01-14 19:31:33,322] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
schema-registry  | [2025-01-14 19:31:33,323] INFO Kafka version: 7.3.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,323] INFO Kafka commitId: a453cbd27246f7bb (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,323] INFO Kafka startTimeMs: 1736883093322 (org.apache.kafka.common.utils.AppInfoParser)
debezium         | 2025-01-14 19:31:33,455 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-vitess/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,455 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-db2   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
zookeeper        | [2025-01-14 19:31:33,466] INFO Processing srvr command from /172.18.0.3:50166 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:31:33,496 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-db2/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,497 INFO   ||  Loading plugin from: /kafka/connect/debezium-connector-mariadb   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
rest-proxy       | [2025-01-14 19:31:33,575] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
schema-registry  | [2025-01-14 19:31:33,629] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,635] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:31:33,635] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:31:33,635] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:31:33,641] INFO Registering schema provider for AVRO: io.confluent.kafka.schemaregistry.avro.AvroSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry  | [2025-01-14 19:31:33,642] INFO Registering schema provider for JSON: io.confluent.kafka.schemaregistry.json.JsonSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry  | [2025-01-14 19:31:33,642] INFO Registering schema provider for PROTOBUF: io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
debezium         | 2025-01-14 19:31:33,656 INFO   ||  Registered loader: PluginClassLoader{pluginLocation=file:/kafka/connect/debezium-connector-mariadb/}   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:33,656 INFO   ||  Loading plugin from: classpath   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
schema-registry  | [2025-01-14 19:31:33,657] INFO Initializing KafkaStore with broker endpoints: PLAINTEXT://broker:29092 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:31:33,658] INFO AdminClientConfig values: 
schema-registry  | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry  | 	client.dns.lookup = use_all_dns_ips
schema-registry  | 	client.id = 
schema-registry  | 	confluent.use.controller.listener = false
schema-registry  | 	connections.max.idle.ms = 300000
schema-registry  | 	default.api.timeout.ms = 60000
schema-registry  | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
schema-registry  | 	metadata.max.age.ms = 300000
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.recording.level = INFO
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	receive.buffer.bytes = 65536
schema-registry  | 	reconnect.backoff.max.ms = 1000
schema-registry  | 	reconnect.backoff.ms = 50
schema-registry  | 	request.timeout.ms = 30000
schema-registry  | 	retries = 2147483647
schema-registry  | 	retry.backoff.ms = 100
schema-registry  | 	sasl.client.callback.handler.class = null
schema-registry  | 	sasl.jaas.config = null
schema-registry  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	sasl.kerberos.service.name = null
schema-registry  | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	sasl.login.callback.handler.class = null
schema-registry  | 	sasl.login.class = null
schema-registry  | 	sasl.login.connect.timeout.ms = null
schema-registry  | 	sasl.login.read.timeout.ms = null
schema-registry  | 	sasl.login.refresh.buffer.seconds = 300
schema-registry  | 	sasl.login.refresh.min.period.seconds = 60
schema-registry  | 	sasl.login.refresh.window.factor = 0.8
schema-registry  | 	sasl.login.refresh.window.jitter = 0.05
schema-registry  | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.login.retry.backoff.ms = 100
schema-registry  | 	sasl.mechanism = GSSAPI
schema-registry  | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry  | 	sasl.oauthbearer.expected.audience = null
schema-registry  | 	sasl.oauthbearer.expected.issuer = null
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry  | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry  | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry  | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry  | 	security.protocol = PLAINTEXT
schema-registry  | 	security.providers = null
schema-registry  | 	send.buffer.bytes = 131072
schema-registry  | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry  | 	socket.connection.setup.timeout.ms = 10000
schema-registry  | 	ssl.cipher.suites = null
schema-registry  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry  | 	ssl.endpoint.identification.algorithm = https
schema-registry  | 	ssl.engine.factory.class = null
schema-registry  | 	ssl.key.password = null
schema-registry  | 	ssl.keymanager.algorithm = SunX509
schema-registry  | 	ssl.keystore.certificate.chain = null
schema-registry  | 	ssl.keystore.key = null
schema-registry  | 	ssl.keystore.location = null
schema-registry  | 	ssl.keystore.password = null
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.protocol = TLSv1.3
schema-registry  | 	ssl.provider = null
schema-registry  | 	ssl.secure.random.implementation = null
schema-registry  | 	ssl.trustmanager.algorithm = PKIX
schema-registry  | 	ssl.truststore.certificates = null
schema-registry  | 	ssl.truststore.location = null
schema-registry  | 	ssl.truststore.password = null
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  |  (org.apache.kafka.clients.admin.AdminClientConfig)
schema-registry  | [2025-01-14 19:31:33,662] INFO Kafka version: 7.3.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,662] INFO Kafka commitId: a453cbd27246f7bb (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,662] INFO Kafka startTimeMs: 1736883093662 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,711] INFO Creating schemas topic _schemas (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:31:33,713] WARN Creating the schema topic _schemas using a replication factor of 1, which is less than the desired one of 3. If this is a production environment, it's crucial to add more brokers and increase the replication factor of the topic. (io.confluent.kafka.schemaregistry.storage.KafkaStore)
broker           | [2025-01-14 19:31:33,765] INFO Creating topic _schemas with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
broker           | [2025-01-14 19:31:33,801] INFO [Controller id=1] New topics: [Set(_schemas)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(_schemas,Some(0R3ds3geT06LexH5VJXTKQ),Map(_schemas-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:33,802] INFO [Controller id=1] New partition creation callback for _schemas-0 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:33,804] INFO [Controller id=1 epoch=1] Changed partition _schemas-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:33,805] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,809] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _schemas-0 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:33,810] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,834] INFO [Controller id=1 epoch=1] Changed partition _schemas-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:33,835] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition _schemas-0 (state.change.logger)
broker           | [2025-01-14 19:31:33,835] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,836] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,837] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _schemas-0 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:33,837] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,840] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,841] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 1 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:33,858] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition _schemas-0 (state.change.logger)
broker           | [2025-01-14 19:31:33,859] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:31:33,859] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
rest-proxy       | [2025-01-14 19:31:33,878] INFO Started o.e.j.s.ServletContextHandler@680bddf5{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
rest-proxy       | [2025-01-14 19:31:33,886] INFO Started o.e.j.s.ServletContextHandler@5f574cc2{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
broker           | [2025-01-14 19:31:33,898] INFO [LogLoader partition=_schemas-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
rest-proxy       | [2025-01-14 19:31:33,905] INFO Started NetworkTrafficServerConnector@6c2ed0cd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8082} (org.eclipse.jetty.server.AbstractConnector)
rest-proxy       | [2025-01-14 19:31:33,906] INFO Started @1536ms (org.eclipse.jetty.server.Server)
rest-proxy       | [2025-01-14 19:31:33,906] INFO Server started, listening for requests... (io.confluent.kafkarest.KafkaRestMain)
broker           | [2025-01-14 19:31:33,906] INFO Created log for partition _schemas-0 in /var/lib/kafka/data/_schemas-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:33,907] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:33,907] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:33,908] INFO [Broker id=1] Leader _schemas-0 with topic id Some(0R3ds3geT06LexH5VJXTKQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:33,914] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition _schemas-0 (state.change.logger)
broker           | [2025-01-14 19:31:33,917] INFO [Broker id=1] Finished LeaderAndIsr request in 78ms correlationId 1 from controller 1 for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:33,919] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=0R3ds3geT06LexH5VJXTKQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 1 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:33,921] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _schemas-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
broker           | [2025-01-14 19:31:33,922] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
broker           | [2025-01-14 19:31:33,924] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 2 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
schema-registry  | [2025-01-14 19:31:33,927] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,929] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:31:33,929] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:31:33,929] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:31:33,935] INFO ProducerConfig values: 
schema-registry  | 	acks = -1
schema-registry  | 	batch.size = 16384
schema-registry  | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry  | 	buffer.memory = 33554432
schema-registry  | 	client.dns.lookup = use_all_dns_ips
schema-registry  | 	client.id = producer-1
schema-registry  | 	compression.type = none
schema-registry  | 	connections.max.idle.ms = 540000
schema-registry  | 	delivery.timeout.ms = 120000
schema-registry  | 	enable.idempotence = false
schema-registry  | 	interceptor.classes = []
schema-registry  | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
schema-registry  | 	linger.ms = 0
schema-registry  | 	max.block.ms = 60000
schema-registry  | 	max.in.flight.requests.per.connection = 5
schema-registry  | 	max.request.size = 1048576
schema-registry  | 	metadata.max.age.ms = 300000
schema-registry  | 	metadata.max.idle.ms = 300000
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.recording.level = INFO
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	partitioner.adaptive.partitioning.enable = true
schema-registry  | 	partitioner.availability.timeout.ms = 0
schema-registry  | 	partitioner.class = null
schema-registry  | 	partitioner.ignore.keys = false
schema-registry  | 	receive.buffer.bytes = 32768
schema-registry  | 	reconnect.backoff.max.ms = 1000
schema-registry  | 	reconnect.backoff.ms = 50
schema-registry  | 	request.timeout.ms = 30000
schema-registry  | 	retries = 0
schema-registry  | 	retry.backoff.ms = 100
schema-registry  | 	sasl.client.callback.handler.class = null
schema-registry  | 	sasl.jaas.config = null
schema-registry  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	sasl.kerberos.service.name = null
schema-registry  | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	sasl.login.callback.handler.class = null
schema-registry  | 	sasl.login.class = null
schema-registry  | 	sasl.login.connect.timeout.ms = null
schema-registry  | 	sasl.login.read.timeout.ms = null
schema-registry  | 	sasl.login.refresh.buffer.seconds = 300
schema-registry  | 	sasl.login.refresh.min.period.seconds = 60
schema-registry  | 	sasl.login.refresh.window.factor = 0.8
schema-registry  | 	sasl.login.refresh.window.jitter = 0.05
schema-registry  | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.login.retry.backoff.ms = 100
schema-registry  | 	sasl.mechanism = GSSAPI
schema-registry  | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry  | 	sasl.oauthbearer.expected.audience = null
schema-registry  | 	sasl.oauthbearer.expected.issuer = null
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry  | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry  | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry  | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry  | 	security.protocol = PLAINTEXT
schema-registry  | 	security.providers = null
schema-registry  | 	send.buffer.bytes = 131072
schema-registry  | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry  | 	socket.connection.setup.timeout.ms = 10000
schema-registry  | 	ssl.cipher.suites = null
schema-registry  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry  | 	ssl.endpoint.identification.algorithm = https
schema-registry  | 	ssl.engine.factory.class = null
schema-registry  | 	ssl.key.password = null
schema-registry  | 	ssl.keymanager.algorithm = SunX509
schema-registry  | 	ssl.keystore.certificate.chain = null
schema-registry  | 	ssl.keystore.key = null
schema-registry  | 	ssl.keystore.location = null
schema-registry  | 	ssl.keystore.password = null
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.protocol = TLSv1.3
schema-registry  | 	ssl.provider = null
schema-registry  | 	ssl.secure.random.implementation = null
schema-registry  | 	ssl.trustmanager.algorithm = PKIX
schema-registry  | 	ssl.truststore.certificates = null
schema-registry  | 	ssl.truststore.location = null
schema-registry  | 	ssl.truststore.password = null
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  | 	transaction.timeout.ms = 60000
schema-registry  | 	transactional.id = null
schema-registry  | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
schema-registry  |  (org.apache.kafka.clients.producer.ProducerConfig)
schema-registry  | [2025-01-14 19:31:33,947] INFO Kafka version: 7.3.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,947] INFO Kafka commitId: a453cbd27246f7bb (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,947] INFO Kafka startTimeMs: 1736883093947 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,957] INFO [Producer clientId=producer-1] Cluster ID: __R4m4AETriw5M5Mmqky6Q (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:33,962] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
schema-registry  | [2025-01-14 19:31:33,962] INFO Kafka store reader thread starting consumer (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:31:33,966] INFO ConsumerConfig values: 
schema-registry  | 	allow.auto.create.topics = true
schema-registry  | 	auto.commit.interval.ms = 5000
schema-registry  | 	auto.offset.reset = earliest
schema-registry  | 	bootstrap.servers = [PLAINTEXT://broker:29092]
schema-registry  | 	check.crcs = true
schema-registry  | 	client.dns.lookup = use_all_dns_ips
schema-registry  | 	client.id = KafkaStore-reader-_schemas
schema-registry  | 	client.rack = 
schema-registry  | 	connections.max.idle.ms = 540000
schema-registry  | 	default.api.timeout.ms = 60000
schema-registry  | 	enable.auto.commit = false
schema-registry  | 	exclude.internal.topics = true
schema-registry  | 	fetch.max.bytes = 52428800
schema-registry  | 	fetch.max.wait.ms = 500
schema-registry  | 	fetch.min.bytes = 1
schema-registry  | 	group.id = schema-registry-schema-registry-8081
schema-registry  | 	group.instance.id = null
schema-registry  | 	heartbeat.interval.ms = 3000
schema-registry  | 	interceptor.classes = []
schema-registry  | 	internal.leave.group.on.close = true
schema-registry  | 	internal.throw.on.fetch.stable.offset.unsupported = false
schema-registry  | 	isolation.level = read_uncommitted
schema-registry  | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
schema-registry  | 	max.partition.fetch.bytes = 1048576
schema-registry  | 	max.poll.interval.ms = 300000
schema-registry  | 	max.poll.records = 500
schema-registry  | 	metadata.max.age.ms = 300000
schema-registry  | 	metric.reporters = []
schema-registry  | 	metrics.num.samples = 2
schema-registry  | 	metrics.recording.level = INFO
schema-registry  | 	metrics.sample.window.ms = 30000
schema-registry  | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
schema-registry  | 	receive.buffer.bytes = 65536
schema-registry  | 	reconnect.backoff.max.ms = 1000
schema-registry  | 	reconnect.backoff.ms = 50
schema-registry  | 	request.timeout.ms = 30000
schema-registry  | 	retry.backoff.ms = 100
schema-registry  | 	sasl.client.callback.handler.class = null
schema-registry  | 	sasl.jaas.config = null
schema-registry  | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
schema-registry  | 	sasl.kerberos.min.time.before.relogin = 60000
schema-registry  | 	sasl.kerberos.service.name = null
schema-registry  | 	sasl.kerberos.ticket.renew.jitter = 0.05
schema-registry  | 	sasl.kerberos.ticket.renew.window.factor = 0.8
schema-registry  | 	sasl.login.callback.handler.class = null
schema-registry  | 	sasl.login.class = null
schema-registry  | 	sasl.login.connect.timeout.ms = null
schema-registry  | 	sasl.login.read.timeout.ms = null
schema-registry  | 	sasl.login.refresh.buffer.seconds = 300
schema-registry  | 	sasl.login.refresh.min.period.seconds = 60
schema-registry  | 	sasl.login.refresh.window.factor = 0.8
schema-registry  | 	sasl.login.refresh.window.jitter = 0.05
schema-registry  | 	sasl.login.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.login.retry.backoff.ms = 100
schema-registry  | 	sasl.mechanism = GSSAPI
schema-registry  | 	sasl.oauthbearer.clock.skew.seconds = 30
schema-registry  | 	sasl.oauthbearer.expected.audience = null
schema-registry  | 	sasl.oauthbearer.expected.issuer = null
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
schema-registry  | 	sasl.oauthbearer.jwks.endpoint.url = null
schema-registry  | 	sasl.oauthbearer.scope.claim.name = scope
schema-registry  | 	sasl.oauthbearer.sub.claim.name = sub
schema-registry  | 	sasl.oauthbearer.token.endpoint.url = null
schema-registry  | 	security.protocol = PLAINTEXT
schema-registry  | 	security.providers = null
schema-registry  | 	send.buffer.bytes = 131072
schema-registry  | 	session.timeout.ms = 45000
schema-registry  | 	socket.connection.setup.timeout.max.ms = 30000
schema-registry  | 	socket.connection.setup.timeout.ms = 10000
schema-registry  | 	ssl.cipher.suites = null
schema-registry  | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
schema-registry  | 	ssl.endpoint.identification.algorithm = https
schema-registry  | 	ssl.engine.factory.class = null
schema-registry  | 	ssl.key.password = null
schema-registry  | 	ssl.keymanager.algorithm = SunX509
schema-registry  | 	ssl.keystore.certificate.chain = null
schema-registry  | 	ssl.keystore.key = null
schema-registry  | 	ssl.keystore.location = null
schema-registry  | 	ssl.keystore.password = null
schema-registry  | 	ssl.keystore.type = JKS
schema-registry  | 	ssl.protocol = TLSv1.3
schema-registry  | 	ssl.provider = null
schema-registry  | 	ssl.secure.random.implementation = null
schema-registry  | 	ssl.trustmanager.algorithm = PKIX
schema-registry  | 	ssl.truststore.certificates = null
schema-registry  | 	ssl.truststore.location = null
schema-registry  | 	ssl.truststore.password = null
schema-registry  | 	ssl.truststore.type = JKS
schema-registry  | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
schema-registry  |  (org.apache.kafka.clients.consumer.ConsumerConfig)
schema-registry  | [2025-01-14 19:31:33,984] INFO Kafka version: 7.3.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,984] INFO Kafka commitId: a453cbd27246f7bb (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,984] INFO Kafka startTimeMs: 1736883093984 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:33,988] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Cluster ID: __R4m4AETriw5M5Mmqky6Q (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:33,993] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Assigned to partition(s): _schemas-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
schema-registry  | [2025-01-14 19:31:33,995] INFO Seeking to beginning for all partitions (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:31:33,995] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Seeking to earliest offset of partition _schemas-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
schema-registry  | [2025-01-14 19:31:33,995] INFO Initialized last consumed offset to -1 (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:31:33,996] INFO [kafka-store-reader-thread-_schemas]: Starting (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:31:34,007] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to 0R3ds3geT06LexH5VJXTKQ (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,034] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to 0R3ds3geT06LexH5VJXTKQ (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,071] INFO Wait to catch up until the offset at 0 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:31:34,106] INFO Reached offset at 0 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:31:34,106] INFO Joining schema registry with Kafka-based coordination (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry  | [2025-01-14 19:31:34,111] INFO Kafka version: 7.3.1-ce (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:34,111] INFO Kafka commitId: a453cbd27246f7bb (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:34,111] INFO Kafka startTimeMs: 1736883094111 (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:31:34,116] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition _schemas-0 to 0 since the associated topicId changed from null to 0R3ds3geT06LexH5VJXTKQ (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,116] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Cluster ID: __R4m4AETriw5M5Mmqky6Q (org.apache.kafka.clients.Metadata)
broker           | [2025-01-14 19:31:34,121] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
debezium         | 2025-01-14 19:31:34,126 INFO   ||  Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:34,126 INFO   ||  Scanning plugins with ReflectionScanner took 1716 ms   [org.apache.kafka.connect.runtime.isolation.PluginScanner]
debezium         | 2025-01-14 19:31:34,130 WARN   ||  One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
debezium         | file:/kafka/connect/debezium-connector-vitess/	io.debezium.connector.vitess.transforms.FilterTransactionTopicRecords	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-vitess/	io.debezium.connector.vitess.transforms.RemoveField	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-vitess/	io.debezium.connector.vitess.transforms.UseLocalVgtid	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-db2/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-ibmi/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-informix/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-jdbc/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-mariadb/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-mongodb/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-mysql/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-oracle/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-postgres/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-spanner/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-sqlserver/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-vitess/	io.debezium.transforms.ContentBasedRouter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-db2/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-ibmi/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-informix/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-jdbc/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-mariadb/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-mongodb/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-mysql/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-oracle/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-postgres/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-spanner/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-sqlserver/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | file:/kafka/connect/debezium-connector-vitess/	io.debezium.transforms.Filter	transformation	2.7.4.Final
debezium         | ]
debezium         | Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config.   [org.apache.kafka.connect.runtime.isolation.Plugins]
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(rbPOKTI2SsKHzcC1tcoy2g),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,132] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
debezium         | 2025-01-14 19:31:34,133 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Filter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,133 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,133 INFO   ||  Added plugin 'io.debezium.connector.vitess.transforms.FilterTransactionTopicRecords'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,134 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.DoubleConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,134 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.DropHeaders'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
debezium         | 2025-01-14 19:31:34,134 INFO   ||  Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
debezium         | 2025-01-14 19:31:34,134 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.InsertHeader'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,134 INFO   ||  Added plugin 'io.debezium.connector.mariadb.MariaDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.connector.postgresql.transforms.timescaledb.TimescaleDb'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.transforms.ExtractNewRecordState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.converters.ByteArrayConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.transforms.outbox.EventRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Cast$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.connector.db2as400.smt.RepackageJavaFriendlySchemaRenamer'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.connector.vitess.transforms.UseLocalVgtid'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,133] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,134] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,135] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,136] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
debezium         | 2025-01-14 19:31:34,135 INFO   ||  Added plugin 'io.debezium.connector.postgresql.rest.DebeziumPostgresConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'io.debezium.transforms.HeaderToValue'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.RegexRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'io.debezium.connector.jdbc.transforms.ConvertCloudEventToSaveableForm'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.json.JsonConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'io.debezium.connector.informix.InformixConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'io.debezium.transforms.ExtractChangedRecordState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.storage.StringConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,136 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.BooleanConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'io.debezium.transforms.ContentBasedRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'io.debezium.connector.postgresql.PostgresConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'io.debezium.connector.sqlserver.rest.DebeziumSqlServerConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.ShortConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'io.debezium.connector.spanner.SpannerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'io.debezium.connector.sqlserver.SqlServerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,137 INFO   ||  Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,138 INFO   ||  Added plugin 'io.debezium.connector.mariadb.rest.DebeziumMariaDbConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,138 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Cast$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,138 INFO   ||  Added plugin 'io.debezium.converters.BinaryDataConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,138 INFO   ||  Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,138 INFO   ||  Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,138 INFO   ||  Added plugin 'io.debezium.connector.mongodb.MongoDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.transforms.partitions.PartitionRouting'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.connector.jdbc.JdbcSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.transforms.Filter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.connector.vitess.VitessConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.IntegerConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.connector.mysql.MySqlConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.FloatConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.connector.oracle.OracleConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,139 INFO   ||  Added plugin 'io.debezium.transforms.SchemaChangeEventFilter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.connector.db2.Db2Connector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.transforms.ByLogicalTableRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ValueToKey'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.converters.LongConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.connector.db2as400.As400RpcConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.transforms.TimezoneConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.connector.vitess.transforms.RemoveField'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.connector.oracle.rest.DebeziumOracleConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,140 INFO   ||  Added plugin 'io.debezium.converters.CloudEventsConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,141 INFO   ||  Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,143 INFO   ||  Added alias 'VitessConnector' to plugin 'io.debezium.connector.vitess.VitessConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'As400RpcConnector' to plugin 'io.debezium.connector.db2as400.As400RpcConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'ContentBasedRouter' to plugin 'io.debezium.transforms.ContentBasedRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'RepackageJavaFriendlySchemaRenamer' to plugin 'io.debezium.connector.db2as400.smt.RepackageJavaFriendlySchemaRenamer'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'RemoveField' to plugin 'io.debezium.connector.vitess.transforms.RemoveField'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'SqlServerConnector' to plugin 'io.debezium.connector.sqlserver.SqlServerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,144 INFO   ||  Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'DebeziumPostgres' to plugin 'io.debezium.connector.postgresql.rest.DebeziumPostgresConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'DebeziumMariaDb' to plugin 'io.debezium.connector.mariadb.rest.DebeziumMariaDbConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'FilterTransactionTopicRecords' to plugin 'io.debezium.connector.vitess.transforms.FilterTransactionTopicRecords'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'JdbcSinkConnector' to plugin 'io.debezium.connector.jdbc.JdbcSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'DebeziumPostgresConnectRestExtension' to plugin 'io.debezium.connector.postgresql.rest.DebeziumPostgresConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'SpannerConnector' to plugin 'io.debezium.connector.spanner.SpannerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,145 INFO   ||  Added alias 'Postgres' to plugin 'io.debezium.connector.postgresql.PostgresConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'ConvertCloudEventToSaveableForm' to plugin 'io.debezium.connector.jdbc.transforms.ConvertCloudEventToSaveableForm'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'Spanner' to plugin 'io.debezium.connector.spanner.SpannerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'MariaDb' to plugin 'io.debezium.connector.mariadb.MariaDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,146 INFO   ||  Added alias 'DebeziumSqlServerConnectRestExtension' to plugin 'io.debezium.connector.sqlserver.rest.DebeziumSqlServerConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'UseLocalVgtid' to plugin 'io.debezium.connector.vitess.transforms.UseLocalVgtid'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'Oracle' to plugin 'io.debezium.connector.oracle.OracleConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'Informix' to plugin 'io.debezium.connector.informix.InformixConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,147 INFO   ||  Added alias 'DebeziumMariaDbConnectRestExtension' to plugin 'io.debezium.connector.mariadb.rest.DebeziumMariaDbConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DebeziumOracle' to plugin 'io.debezium.connector.oracle.rest.DebeziumOracleConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'MariaDbConnector' to plugin 'io.debezium.connector.mariadb.MariaDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DebeziumSqlServer' to plugin 'io.debezium.connector.sqlserver.rest.DebeziumSqlServerConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DebeziumOracleConnectRestExtension' to plugin 'io.debezium.connector.oracle.rest.DebeziumOracleConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'Db2' to plugin 'io.debezium.connector.db2.Db2Connector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'Db2Connector' to plugin 'io.debezium.connector.db2.Db2Connector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'Vitess' to plugin 'io.debezium.connector.vitess.VitessConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'InformixConnector' to plugin 'io.debezium.connector.informix.InformixConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'OracleConnector' to plugin 'io.debezium.connector.oracle.OracleConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'SqlServer' to plugin 'io.debezium.connector.sqlserver.SqlServerConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,148 INFO   ||  Added alias 'JdbcSink' to plugin 'io.debezium.connector.jdbc.JdbcSinkConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,149 INFO   ||  Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,149 INFO   ||  Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,149 INFO   ||  Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,149 INFO   ||  Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,149 INFO   ||  Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'TimescaleDb' to plugin 'io.debezium.connector.postgresql.transforms.timescaledb.TimescaleDb'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'PostgresConnector' to plugin 'io.debezium.connector.postgresql.PostgresConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,150 INFO   ||  Added alias 'As400Rpc' to plugin 'io.debezium.connector.db2as400.As400RpcConnector'   [org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader]
debezium         | 2025-01-14 19:31:34,168 INFO   ||  DistributedConfig values: 
debezium         | 	access.control.allow.methods = 
debezium         | 	access.control.allow.origin = 
debezium         | 	admin.listeners = null
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 
debezium         | 	config.providers = []
debezium         | 	config.storage.replication.factor = 1
debezium         | 	config.storage.topic = connect_configs
debezium         | 	connect.protocol = sessioned
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	connector.client.config.override.policy = All
debezium         | 	exactly.once.source.support = disabled
debezium         | 	group.id = 1
debezium         | 	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
debezium         | 	heartbeat.interval.ms = 3000
debezium         | 	inter.worker.key.generation.algorithm = HmacSHA256
debezium         | 	inter.worker.key.size = null
debezium         | 	inter.worker.key.ttl.ms = 3600000
debezium         | 	inter.worker.signature.algorithm = HmacSHA256
debezium         | 	inter.worker.verification.algorithms = [HmacSHA256]
debezium         | 	key.converter = class org.apache.kafka.connect.json.JsonConverter
debezium         | 	listeners = [http://:8083]
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	offset.flush.interval.ms = 60000
debezium         | 	offset.flush.timeout.ms = 5000
debezium         | 	offset.storage.partitions = 25
debezium         | 	offset.storage.replication.factor = 1
debezium         | 	offset.storage.topic = connect_offsets
debezium         | 	plugin.discovery = hybrid_warn
debezium         | 	plugin.path = [/kafka/connect]
debezium         | 	rebalance.timeout.ms = 60000
debezium         | 	receive.buffer.bytes = 32768
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 40000
debezium         | 	response.http.headers.config = 
debezium         | 	rest.advertised.host.name = 172.18.0.8
debezium         | 	rest.advertised.listener = null
debezium         | 	rest.advertised.port = 8083
debezium         | 	rest.extension.classes = []
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	scheduled.rebalance.max.delay.ms = 300000
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	send.buffer.bytes = 131072
debezium         | 	session.timeout.ms = 10000
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.client.auth = none
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	status.storage.partitions = 5
debezium         | 	status.storage.replication.factor = 1
debezium         | 	status.storage.topic = connect_statuses
debezium         | 	task.shutdown.graceful.timeout.ms = 10000
debezium         | 	topic.creation.enable = true
debezium         | 	topic.tracking.allow.reset = true
debezium         | 	topic.tracking.enable = true
debezium         | 	value.converter = class org.apache.kafka.connect.json.JsonConverter
debezium         | 	worker.sync.timeout.ms = 3000
debezium         | 	worker.unsync.backoff.ms = 300000
debezium         |    [org.apache.kafka.connect.runtime.distributed.DistributedConfig]
debezium         | 2025-01-14 19:31:34,169 INFO   ||  Creating Kafka admin client   [org.apache.kafka.connect.runtime.WorkerConfig]
debezium         | 2025-01-14 19:31:34,172 INFO   ||  AdminClientConfig values: 
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	bootstrap.controllers = []
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 
debezium         | 	connections.max.idle.ms = 300000
debezium         | 	default.api.timeout.ms = 60000
debezium         | 	enable.metrics.push = true
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	receive.buffer.bytes = 65536
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retries = 2147483647
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         |    [org.apache.kafka.clients.admin.AdminClientConfig]
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,197] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,198] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
broker           | [2025-01-14 19:31:34,198] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
broker           | [2025-01-14 19:31:34,198] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
broker           | [2025-01-14 19:31:34,198] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
broker           | [2025-01-14 19:31:34,199] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
broker           | [2025-01-14 19:31:34,200] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,200] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 50 become-leader and 0 become-follower partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,200] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 50 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,201] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,202] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,203] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 for 50 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,203] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,204] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 3 from controller 1 epoch 1 (state.change.logger)
debezium         | 2025-01-14 19:31:34,212 INFO   ||  These configurations '[config.storage.topic, rest.advertised.host.name, status.storage.topic, group.id, rest.advertised.port, rest.host.name, task.shutdown.graceful.timeout.ms, plugin.path, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet.   [org.apache.kafka.clients.admin.AdminClientConfig]
debezium         | 2025-01-14 19:31:34,212 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,212 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,212 INFO   ||  Kafka startTimeMs: 1736883094212   [org.apache.kafka.common.utils.AppInfoParser]
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
broker           | [2025-01-14 19:31:34,217] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
broker           | [2025-01-14 19:31:34,218] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:31:34,219] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,223] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,224] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,225] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,225] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,225] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,233] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,234] INFO Created log for partition __consumer_offsets-18 in /var/lib/kafka/data/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,234] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,234] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,234] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,241] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,242] INFO Created log for partition __consumer_offsets-41 in /var/lib/kafka/data/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,242] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,242] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,242] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,247] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,248] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,248] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,248] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,248] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,254] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,254] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,254] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,254] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,254] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,260] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,260] INFO Created log for partition __consumer_offsets-48 in /var/lib/kafka/data/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,260] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,260] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,260] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,266] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,267] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,267] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,267] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,267] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,273] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,274] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,274] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,274] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,274] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,280] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,281] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,281] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,281] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,281] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,286] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,287] INFO Created log for partition __consumer_offsets-11 in /var/lib/kafka/data/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,287] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,287] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,287] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,292] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,293] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,293] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,293] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,293] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,299] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,299] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,299] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,299] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,299] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,304] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,304] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,304] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,304] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,305] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,310] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,311] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,311] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,311] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,311] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,317] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,318] INFO Created log for partition __consumer_offsets-24 in /var/lib/kafka/data/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,318] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,318] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,318] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,323] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,324] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,324] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,324] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,324] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,330] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,330] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,330] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,330] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,330] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,335] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,336] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,336] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,336] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,336] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,341] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,341] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,341] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,341] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,342] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,347] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,347] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,347] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,347] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,347] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,351] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,352] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,352] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,352] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,352] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,355 INFO   ||  Kafka cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.connect.runtime.WorkerConfig]
debezium         | 2025-01-14 19:31:34,356 INFO   ||  App info kafka.admin.client for adminclient-1 unregistered   [org.apache.kafka.common.utils.AppInfoParser]
broker           | [2025-01-14 19:31:34,357] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,358] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,358] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,358] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,358] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,360 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:31:34,360 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:31:34,360 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:31:34,362 INFO   ||  PublicConfig values: 
debezium         | 	access.control.allow.methods = 
debezium         | 	access.control.allow.origin = 
debezium         | 	admin.listeners = null
debezium         | 	listeners = [http://:8083]
debezium         | 	response.http.headers.config = 
debezium         | 	rest.advertised.host.name = 172.18.0.8
debezium         | 	rest.advertised.listener = null
debezium         | 	rest.advertised.port = 8083
debezium         | 	rest.extension.classes = []
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.client.auth = none
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	topic.tracking.allow.reset = true
debezium         | 	topic.tracking.enable = true
debezium         |    [org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig]
broker           | [2025-01-14 19:31:34,364] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,364] INFO Created log for partition __consumer_offsets-47 in /var/lib/kafka/data/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,364] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,364] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,365] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,367 INFO   ||  Logging initialized @3920ms to org.eclipse.jetty.util.log.Slf4jLog   [org.eclipse.jetty.util.log]
broker           | [2025-01-14 19:31:34,370] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,371] INFO Created log for partition __consumer_offsets-17 in /var/lib/kafka/data/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,371] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,371] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,371] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,379] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,380] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,380] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,380] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,380] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,388] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,388] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,388] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,388] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,388] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,395] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,395] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,395] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,395] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,395] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,399 INFO   ||  Added connector for http://:8083   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,400 INFO   ||  Initializing REST server   [org.apache.kafka.connect.runtime.rest.RestServer]
broker           | [2025-01-14 19:31:34,401] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,401] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,401] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,401] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,401] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,407] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,407] INFO Created log for partition __consumer_offsets-29 in /var/lib/kafka/data/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,407] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,407] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,407] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,413 INFO   ||  jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 21.0.5+11   [org.eclipse.jetty.server.Server]
broker           | [2025-01-14 19:31:34,414] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,414] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,414] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,414] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,414] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,420] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,420] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,420] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,420] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,420] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,428 INFO   ||  Started http_8083@5f25b34d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083}   [org.eclipse.jetty.server.AbstractConnector]
broker           | [2025-01-14 19:31:34,428] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
debezium         | 2025-01-14 19:31:34,428 INFO   ||  Started @3982ms   [org.eclipse.jetty.server.Server]
broker           | [2025-01-14 19:31:34,428] INFO Created log for partition __consumer_offsets-23 in /var/lib/kafka/data/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,428] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,428] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,428] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,434] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,434] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,435] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,435] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,435] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,437 INFO   ||  Advertised URI: http://172.18.0.8:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,437 INFO   ||  REST server listening at http://172.18.0.8:8083/, advertising URL http://172.18.0.8:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,437 INFO   ||  Advertised URI: http://172.18.0.8:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,437 INFO   ||  REST admin endpoints at http://172.18.0.8:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,437 INFO   ||  Advertised URI: http://172.18.0.8:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,438 INFO   ||  Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden   [org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy]
debezium         | 2025-01-14 19:31:34,440 INFO   ||  JsonConverterConfig values: 
debezium         | 	converter.type = key
debezium         | 	decimal.format = BASE64
debezium         | 	replace.null.with.default = true
debezium         | 	schemas.cache.size = 1000
debezium         | 	schemas.enable = false
debezium         |    [org.apache.kafka.connect.json.JsonConverterConfig]
broker           | [2025-01-14 19:31:34,441] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,442] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,442] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,442] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,442] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,447] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,447] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,447] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,447] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,448] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,448 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,448 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,448 INFO   ||  Kafka startTimeMs: 1736883094448   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,451 INFO   ||  JsonConverterConfig values: 
debezium         | 	converter.type = key
debezium         | 	decimal.format = BASE64
debezium         | 	replace.null.with.default = true
debezium         | 	schemas.cache.size = 1000
debezium         | 	schemas.enable = false
debezium         |    [org.apache.kafka.connect.json.JsonConverterConfig]
debezium         | 2025-01-14 19:31:34,451 INFO   ||  JsonConverterConfig values: 
debezium         | 	converter.type = value
debezium         | 	decimal.format = BASE64
debezium         | 	replace.null.with.default = true
debezium         | 	schemas.cache.size = 1000
debezium         | 	schemas.enable = false
debezium         |    [org.apache.kafka.connect.json.JsonConverterConfig]
broker           | [2025-01-14 19:31:34,453] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,454] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,454] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,454] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,454] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,459 INFO   ||  Advertised URI: http://172.18.0.8:8083/   [org.apache.kafka.connect.runtime.rest.RestServer]
broker           | [2025-01-14 19:31:34,461] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,461] INFO Created log for partition __consumer_offsets-30 in /var/lib/kafka/data/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,461] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,461] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,462] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,467] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,468] INFO Created log for partition __consumer_offsets-0 in /var/lib/kafka/data/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,468] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,468] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,468] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,472 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,472 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,472 INFO   ||  Kafka startTimeMs: 1736883094472   [org.apache.kafka.common.utils.AppInfoParser]
broker           | [2025-01-14 19:31:34,473] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,473] INFO Created log for partition __consumer_offsets-35 in /var/lib/kafka/data/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,473] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,473] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,473] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,474 INFO   ||  Kafka Connect worker initialization took 3783ms   [org.apache.kafka.connect.cli.AbstractConnectCli]
debezium         | 2025-01-14 19:31:34,474 INFO   ||  Kafka Connect starting   [org.apache.kafka.connect.runtime.Connect]
debezium         | 2025-01-14 19:31:34,475 INFO   ||  Initializing REST resources   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,475 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Herder starting   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:31:34,476 INFO   ||  Worker starting   [org.apache.kafka.connect.runtime.Worker]
debezium         | 2025-01-14 19:31:34,476 INFO   ||  Starting KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
debezium         | 2025-01-14 19:31:34,476 INFO   ||  Starting KafkaBasedLog with topic connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:34,477 INFO   ||  AdminClientConfig values: 
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	bootstrap.controllers = []
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-shared-admin
debezium         | 	connections.max.idle.ms = 300000
debezium         | 	default.api.timeout.ms = 60000
debezium         | 	enable.metrics.push = true
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	receive.buffer.bytes = 65536
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retries = 2147483647
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         |    [org.apache.kafka.clients.admin.AdminClientConfig]
debezium         | 2025-01-14 19:31:34,479 INFO   ||  These configurations '[config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, group.id, rest.advertised.port, rest.host.name, task.shutdown.graceful.timeout.ms, plugin.path, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet.   [org.apache.kafka.clients.admin.AdminClientConfig]
debezium         | 2025-01-14 19:31:34,479 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,479 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,479 INFO   ||  Kafka startTimeMs: 1736883094479   [org.apache.kafka.common.utils.AppInfoParser]
broker           | [2025-01-14 19:31:34,480] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,481] INFO Created log for partition __consumer_offsets-5 in /var/lib/kafka/data/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,481] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,481] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,481] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,492] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,493] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,493] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,493] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,493] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,495] INFO Creating topic connect_offsets with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
debezium         | 2025-01-14 19:31:34,496 INFO   ||  Adding admin resources to main listener   [org.apache.kafka.connect.runtime.rest.RestServer]
broker           | [2025-01-14 19:31:34,499] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,500] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,500] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,500] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,500] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,503] INFO [Controller id=1] New topics: [Set(connect_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect_offsets,Some(a-LLSVYSQWeIP3xO6wpKeQ),HashMap(connect_offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1] New partition creation callback for connect_offsets-20,connect_offsets-5,connect_offsets-13,connect_offsets-23,connect_offsets-22,connect_offsets-12,connect_offsets-21,connect_offsets-0,connect_offsets-14,connect_offsets-1,connect_offsets-19,connect_offsets-15,connect_offsets-11,connect_offsets-8,connect_offsets-24,connect_offsets-16,connect_offsets-6,connect_offsets-2,connect_offsets-9,connect_offsets-17,connect_offsets-4,connect_offsets-3,connect_offsets-7,connect_offsets-18,connect_offsets-10 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,504] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,506] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,507] INFO Created log for partition __consumer_offsets-42 in /var/lib/kafka/data/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,507] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,507] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,507] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,516 INFO   ||  DefaultSessionIdManager workerName=node0   [org.eclipse.jetty.server.session]
debezium         | 2025-01-14 19:31:34,516 INFO   ||  No SessionScavenger set, using defaults   [org.eclipse.jetty.server.session]
broker           | [2025-01-14 19:31:34,516] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,517] INFO Created log for partition __consumer_offsets-12 in /var/lib/kafka/data/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,517] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,517] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,517] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
debezium         | 2025-01-14 19:31:34,517 INFO   ||  node0 Scavenging every 600000ms   [org.eclipse.jetty.server.session]
broker           | [2025-01-14 19:31:34,526] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,527] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,527] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,527] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,527] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,527] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] INFO [Controller id=1 epoch=1] Changed partition connect_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-5 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-7 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-9 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-11 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-14 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-16 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-18 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-20 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-22 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-24 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-6 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-8 (state.change.logger)
broker           | [2025-01-14 19:31:34,528] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-10 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-12 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-13 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-15 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-17 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-19 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-21 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_offsets-23 (state.change.logger)
broker           | [2025-01-14 19:31:34,529] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 25 become-leader and 0 become-follower partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,529] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 25 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,530] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,535] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,535] INFO Created log for partition __consumer_offsets-36 in /var/lib/kafka/data/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,535] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,535] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,535] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,540] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,541] INFO Created log for partition __consumer_offsets-6 in /var/lib/kafka/data/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,541] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,541] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,541] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,546] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,546] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,546] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,546] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,546] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,551] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,551] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,551] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,551] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,551] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,557] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,558] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,558] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,558] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,558] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(rbPOKTI2SsKHzcC1tcoy2g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
broker           | [2025-01-14 19:31:34,561] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
broker           | [2025-01-14 19:31:34,562] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,563] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,564] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,564] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,564] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,564] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,564] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,564] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,564] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,564] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,564] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,564] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,565] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,565] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,566] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,566] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,567] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,567] INFO [Broker id=1] Finished LeaderAndIsr request in 364ms correlationId 3 from controller 1 for 50 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,569] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=rbPOKTI2SsKHzcC1tcoy2g, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=13, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=46, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=9, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=42, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=21, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=17, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=30, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=26, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=5, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=38, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=1, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=34, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=16, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=45, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=12, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=41, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=24, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=20, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=49, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=29, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=25, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=8, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=37, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=4, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=33, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=15, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=48, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=11, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=44, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=23, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=19, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=32, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=28, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=7, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=40, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=3, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=36, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=47, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=14, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=43, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=10, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=22, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=18, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=31, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=27, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=39, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=6, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=35, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=2, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 3 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] INFO [Broker id=1] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
broker           | [2025-01-14 19:31:34,570] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 7 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,571] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,571] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,571] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,571] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,571] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 4 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:34,571] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 7 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,572] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 for 25 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,573] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,574] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,574] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,574] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,574] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 5 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,574] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,575] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,575] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-19 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-15 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-11 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-22 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-7 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-20 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-5 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-16 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-12 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-8 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-23 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-21 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-17 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-13 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-24 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-9 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-18 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-14 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-10 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition connect_offsets-6 (state.change.logger)
broker           | [2025-01-14 19:31:34,577] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(connect_offsets-20, connect_offsets-5, connect_offsets-13, connect_offsets-23, connect_offsets-22, connect_offsets-12, connect_offsets-21, connect_offsets-0, connect_offsets-14, connect_offsets-1, connect_offsets-19, connect_offsets-15, connect_offsets-11, connect_offsets-8, connect_offsets-24, connect_offsets-16, connect_offsets-6, connect_offsets-2, connect_offsets-9, connect_offsets-17, connect_offsets-4, connect_offsets-3, connect_offsets-7, connect_offsets-18, connect_offsets-10) (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:31:34,577] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 1 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,581] INFO [LogLoader partition=connect_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,581] INFO Created log for partition connect_offsets-4 in /var/lib/kafka/data/connect_offsets-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,582] INFO [Partition connect_offsets-4 broker=1] No checkpointed highwatermark is found for partition connect_offsets-4 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,582] INFO [Partition connect_offsets-4 broker=1] Log loaded for partition connect_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,582] INFO [Broker id=1] Leader connect_offsets-4 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,587] INFO [LogLoader partition=connect_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,588] INFO Created log for partition connect_offsets-19 in /var/lib/kafka/data/connect_offsets-19 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,588] INFO [Partition connect_offsets-19 broker=1] No checkpointed highwatermark is found for partition connect_offsets-19 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,588] INFO [Partition connect_offsets-19 broker=1] Log loaded for partition connect_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,588] INFO [Broker id=1] Leader connect_offsets-19 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,593] INFO [LogLoader partition=connect_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,593] INFO Created log for partition connect_offsets-0 in /var/lib/kafka/data/connect_offsets-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,593] INFO [Partition connect_offsets-0 broker=1] No checkpointed highwatermark is found for partition connect_offsets-0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,593] INFO [Partition connect_offsets-0 broker=1] Log loaded for partition connect_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,593] INFO [Broker id=1] Leader connect_offsets-0 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,599] INFO [LogLoader partition=connect_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,599] INFO Created log for partition connect_offsets-15 in /var/lib/kafka/data/connect_offsets-15 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,599] INFO [Partition connect_offsets-15 broker=1] No checkpointed highwatermark is found for partition connect_offsets-15 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,599] INFO [Partition connect_offsets-15 broker=1] Log loaded for partition connect_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,599] INFO [Broker id=1] Leader connect_offsets-15 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,604] INFO [LogLoader partition=connect_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,604] INFO Created log for partition connect_offsets-11 in /var/lib/kafka/data/connect_offsets-11 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,604] INFO [Partition connect_offsets-11 broker=1] No checkpointed highwatermark is found for partition connect_offsets-11 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,604] INFO [Partition connect_offsets-11 broker=1] Log loaded for partition connect_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,604] INFO [Broker id=1] Leader connect_offsets-11 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,609] INFO [LogLoader partition=connect_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,609] INFO Created log for partition connect_offsets-22 in /var/lib/kafka/data/connect_offsets-22 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,610] INFO [Partition connect_offsets-22 broker=1] No checkpointed highwatermark is found for partition connect_offsets-22 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,610] INFO [Partition connect_offsets-22 broker=1] Log loaded for partition connect_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,610] INFO [Broker id=1] Leader connect_offsets-22 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,615] INFO [LogLoader partition=connect_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,615] INFO Created log for partition connect_offsets-7 in /var/lib/kafka/data/connect_offsets-7 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,615] INFO [Partition connect_offsets-7 broker=1] No checkpointed highwatermark is found for partition connect_offsets-7 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,615] INFO [Partition connect_offsets-7 broker=1] Log loaded for partition connect_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,615] INFO [Broker id=1] Leader connect_offsets-7 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,621] INFO [LogLoader partition=connect_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,622] INFO Created log for partition connect_offsets-20 in /var/lib/kafka/data/connect_offsets-20 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,622] INFO [Partition connect_offsets-20 broker=1] No checkpointed highwatermark is found for partition connect_offsets-20 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,622] INFO [Partition connect_offsets-20 broker=1] Log loaded for partition connect_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,622] INFO [Broker id=1] Leader connect_offsets-20 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,629] INFO [LogLoader partition=connect_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
schema-registry  | [2025-01-14 19:31:34,629] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-0 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-10 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-20 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-40 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-30 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-9 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
broker           | [2025-01-14 19:31:34,630] INFO Created log for partition connect_offsets-5 in /var/lib/kafka/data/connect_offsets-5 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,630] INFO [Partition connect_offsets-5 broker=1] No checkpointed highwatermark is found for partition connect_offsets-5 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,630] INFO [Partition connect_offsets-5 broker=1] Log loaded for partition connect_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,630] INFO [Broker id=1] Leader connect_offsets-5 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-11 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-31 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-39 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-13 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-18 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-22 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-8 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-32 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-43 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-29 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-34 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-1 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-6 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-41 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-27 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-48 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,630] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-5 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-15 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-35 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-25 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-46 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-26 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-36 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-44 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-16 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-37 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-17 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-45 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-3 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-24 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-38 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-33 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,631] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-23 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-28 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-2 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-12 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-19 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-14 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-4 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-47 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-49 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-42 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-7 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,632] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting the last seen epoch of partition __consumer_offsets-21 to 0 since the associated topicId changed from null to rbPOKTI2SsKHzcC1tcoy2g (org.apache.kafka.clients.Metadata)
schema-registry  | [2025-01-14 19:31:34,636] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker           | [2025-01-14 19:31:34,638] INFO [LogLoader partition=connect_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,638] INFO Created log for partition connect_offsets-16 in /var/lib/kafka/data/connect_offsets-16 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,638] INFO [Partition connect_offsets-16 broker=1] No checkpointed highwatermark is found for partition connect_offsets-16 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,638] INFO [Partition connect_offsets-16 broker=1] Log loaded for partition connect_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,638] INFO [Broker id=1] Leader connect_offsets-16 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry  | [2025-01-14 19:31:34,639] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker           | [2025-01-14 19:31:34,645] INFO [LogLoader partition=connect_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,645] INFO Created log for partition connect_offsets-1 in /var/lib/kafka/data/connect_offsets-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,645] INFO [Partition connect_offsets-1 broker=1] No checkpointed highwatermark is found for partition connect_offsets-1 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,646] INFO [Partition connect_offsets-1 broker=1] Log loaded for partition connect_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,646] INFO [Broker id=1] Leader connect_offsets-1 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,651] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group schema-registry in Empty state. Created a new member id sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,652] INFO [LogLoader partition=connect_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,653] INFO Created log for partition connect_offsets-12 in /var/lib/kafka/data/connect_offsets-12 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,653] INFO [Partition connect_offsets-12 broker=1] No checkpointed highwatermark is found for partition connect_offsets-12 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,653] INFO [Partition connect_offsets-12 broker=1] Log loaded for partition connect_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,653] INFO [Broker id=1] Leader connect_offsets-12 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry  | [2025-01-14 19:31:34,655] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: need to re-join with the given member-id: sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117 (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:31:34,655] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException) (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:31:34,655] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker           | [2025-01-14 19:31:34,659] INFO [LogLoader partition=connect_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,659] INFO Created log for partition connect_offsets-8 in /var/lib/kafka/data/connect_offsets-8 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,660] INFO [Partition connect_offsets-8 broker=1] No checkpointed highwatermark is found for partition connect_offsets-8 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,660] INFO [Partition connect_offsets-8 broker=1] Log loaded for partition connect_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,660] INFO [Broker id=1] Leader connect_offsets-8 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,661] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117 with group instance id None; client reason: rebalance failed due to MemberIdRequiredException) (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,664] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) with 1 members (kafka.coordinator.group.GroupCoordinator)
schema-registry  | [2025-01-14 19:31:34,666] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Successfully joined group with generation Generation{generationId=1, memberId='sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117', protocol='v0'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
broker           | [2025-01-14 19:31:34,667] INFO [LogLoader partition=connect_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,667] INFO Created log for partition connect_offsets-23 in /var/lib/kafka/data/connect_offsets-23 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,667] INFO [Partition connect_offsets-23 broker=1] No checkpointed highwatermark is found for partition connect_offsets-23 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,667] INFO [Partition connect_offsets-23 broker=1] Log loaded for partition connect_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,667] INFO [Broker id=1] Leader connect_offsets-23 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,676] INFO [LogLoader partition=connect_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,676] INFO [GroupCoordinator 1]: Assignment received from leader sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117 for group schema-registry for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:34,676] INFO Created log for partition connect_offsets-21 in /var/lib/kafka/data/connect_offsets-21 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,676] INFO [Partition connect_offsets-21 broker=1] No checkpointed highwatermark is found for partition connect_offsets-21 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,676] INFO [Partition connect_offsets-21 broker=1] Log loaded for partition connect_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,676] INFO [Broker id=1] Leader connect_offsets-21 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,683] INFO [LogLoader partition=connect_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,684] INFO Created log for partition connect_offsets-2 in /var/lib/kafka/data/connect_offsets-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,684] INFO [Partition connect_offsets-2 broker=1] No checkpointed highwatermark is found for partition connect_offsets-2 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,684] INFO [Partition connect_offsets-2 broker=1] Log loaded for partition connect_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,684] INFO [Broker id=1] Leader connect_offsets-2 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry  | [2025-01-14 19:31:34,688] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Successfully synced group in generation Generation{generationId=1, memberId='sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117', protocol='v0'} (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:31:34,690] INFO Finished rebalance with leader election result: Assignment{version=1, error=0, leader='sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117', leaderIdentity=version=1,host=schema-registry,port=8081,scheme=http,leaderEligibility=true} (io.confluent.kafka.schemaregistry.leaderelector.kafka.KafkaGroupLeaderElector)
broker           | [2025-01-14 19:31:34,692] INFO [LogLoader partition=connect_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,692] INFO Created log for partition connect_offsets-17 in /var/lib/kafka/data/connect_offsets-17 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,692] INFO [Partition connect_offsets-17 broker=1] No checkpointed highwatermark is found for partition connect_offsets-17 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,692] INFO [Partition connect_offsets-17 broker=1] Log loaded for partition connect_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,692] INFO [Broker id=1] Leader connect_offsets-17 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,698] INFO [LogLoader partition=connect_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,698] INFO Created log for partition connect_offsets-13 in /var/lib/kafka/data/connect_offsets-13 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,698] INFO [Partition connect_offsets-13 broker=1] No checkpointed highwatermark is found for partition connect_offsets-13 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,698] INFO [Partition connect_offsets-13 broker=1] Log loaded for partition connect_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,698] INFO [Broker id=1] Leader connect_offsets-13 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry  | [2025-01-14 19:31:34,701] INFO Wait to catch up until the offset at 1 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:31:34,702] INFO Reached offset at 1 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
broker           | [2025-01-14 19:31:34,705] INFO [LogLoader partition=connect_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,705] INFO Created log for partition connect_offsets-24 in /var/lib/kafka/data/connect_offsets-24 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,705] INFO [Partition connect_offsets-24 broker=1] No checkpointed highwatermark is found for partition connect_offsets-24 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,705] INFO [Partition connect_offsets-24 broker=1] Log loaded for partition connect_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,705] INFO [Broker id=1] Leader connect_offsets-24 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,711] INFO [LogLoader partition=connect_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,711] INFO Created log for partition connect_offsets-9 in /var/lib/kafka/data/connect_offsets-9 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,711] INFO [Partition connect_offsets-9 broker=1] No checkpointed highwatermark is found for partition connect_offsets-9 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,711] INFO [Partition connect_offsets-9 broker=1] Log loaded for partition connect_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,711] INFO [Broker id=1] Leader connect_offsets-9 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,717] INFO [LogLoader partition=connect_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,718] INFO Created log for partition connect_offsets-18 in /var/lib/kafka/data/connect_offsets-18 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,718] INFO [Partition connect_offsets-18 broker=1] No checkpointed highwatermark is found for partition connect_offsets-18 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,718] INFO [Partition connect_offsets-18 broker=1] Log loaded for partition connect_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,718] INFO [Broker id=1] Leader connect_offsets-18 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,723] INFO [LogLoader partition=connect_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,724] INFO Created log for partition connect_offsets-3 in /var/lib/kafka/data/connect_offsets-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,724] INFO [Partition connect_offsets-3 broker=1] No checkpointed highwatermark is found for partition connect_offsets-3 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,724] INFO [Partition connect_offsets-3 broker=1] Log loaded for partition connect_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,724] INFO [Broker id=1] Leader connect_offsets-3 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,731] INFO [LogLoader partition=connect_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,731] INFO Created log for partition connect_offsets-14 in /var/lib/kafka/data/connect_offsets-14 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,731] INFO [Partition connect_offsets-14 broker=1] No checkpointed highwatermark is found for partition connect_offsets-14 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,731] INFO [Partition connect_offsets-14 broker=1] Log loaded for partition connect_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,731] INFO [Broker id=1] Leader connect_offsets-14 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
schema-registry  | [2025-01-14 19:31:34,732] INFO Binding SchemaRegistryRestApplication to all listeners. (io.confluent.rest.Application)
broker           | [2025-01-14 19:31:34,737] INFO [LogLoader partition=connect_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,738] INFO Created log for partition connect_offsets-10 in /var/lib/kafka/data/connect_offsets-10 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,738] INFO [Partition connect_offsets-10 broker=1] No checkpointed highwatermark is found for partition connect_offsets-10 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,738] INFO [Partition connect_offsets-10 broker=1] Log loaded for partition connect_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,738] INFO [Broker id=1] Leader connect_offsets-10 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,745] INFO [LogLoader partition=connect_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,745] INFO Created log for partition connect_offsets-6 in /var/lib/kafka/data/connect_offsets-6 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,745] INFO [Partition connect_offsets-6 broker=1] No checkpointed highwatermark is found for partition connect_offsets-6 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,745] INFO [Partition connect_offsets-6 broker=1] Log loaded for partition connect_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,745] INFO [Broker id=1] Leader connect_offsets-6 with topic id Some(a-LLSVYSQWeIP3xO6wpKeQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-19 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-15 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-11 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-22 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-7 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-20 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-5 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-16 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-12 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-8 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-23 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-21 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-17 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-13 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-24 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-9 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-18 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-14 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-10 (state.change.logger)
broker           | [2025-01-14 19:31:34,748] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition connect_offsets-6 (state.change.logger)
broker           | [2025-01-14 19:31:34,749] INFO [Broker id=1] Finished LeaderAndIsr request in 176ms correlationId 5 from controller 1 for 25 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,750] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=a-LLSVYSQWeIP3xO6wpKeQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=1, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=3, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=5, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=7, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=9, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=11, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=14, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=16, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=18, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=20, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=22, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=24, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=2, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=4, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=6, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=8, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=10, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=12, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=13, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=15, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=17, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=19, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=21, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=23, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 5 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,751] INFO [Broker id=1] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
broker           | [2025-01-14 19:31:34,752] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 6 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
debezium         | 2025-01-14 19:31:34,754 INFO   ||  Created topic (name=connect_offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092   [org.apache.kafka.connect.util.TopicAdmin]
debezium         | 2025-01-14 19:31:34,758 INFO   ||  ProducerConfig values: 
debezium         | 	acks = -1
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	batch.size = 16384
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	buffer.memory = 33554432
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-offsets
debezium         | 	compression.type = none
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	delivery.timeout.ms = 2147483647
debezium         | 	enable.idempotence = false
debezium         | 	enable.metrics.push = true
debezium         | 	interceptor.classes = []
debezium         | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
debezium         | 	linger.ms = 0
debezium         | 	max.block.ms = 60000
debezium         | 	max.in.flight.requests.per.connection = 1
debezium         | 	max.request.size = 1048576
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metadata.max.idle.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	partitioner.adaptive.partitioning.enable = true
debezium         | 	partitioner.availability.timeout.ms = 0
debezium         | 	partitioner.class = null
debezium         | 	partitioner.ignore.keys = false
debezium         | 	receive.buffer.bytes = 32768
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retries = 2147483647
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	transaction.timeout.ms = 60000
debezium         | 	transactional.id = null
debezium         | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
debezium         |    [org.apache.kafka.clients.producer.ProducerConfig]
debezium         | 2025-01-14 19:31:34,761 INFO   ||  Started o.e.j.s.ServletContextHandler@93bc31f{/,null,AVAILABLE}   [org.eclipse.jetty.server.handler.ContextHandler]
debezium         | 2025-01-14 19:31:34,761 INFO   ||  REST resources initialized; server is started and ready to handle requests   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:31:34,761 INFO   ||  Kafka Connect started   [org.apache.kafka.connect.runtime.Connect]
debezium         | 2025-01-14 19:31:34,770 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
debezium         | 2025-01-14 19:31:34,780 INFO   ||  These configurations '[group.id, rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.producer.ProducerConfig]
debezium         | 2025-01-14 19:31:34,780 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,780 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,780 INFO   ||  Kafka startTimeMs: 1736883094780   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,783 INFO   ||  ConsumerConfig values: 
debezium         | 	allow.auto.create.topics = true
debezium         | 	auto.commit.interval.ms = 5000
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	auto.offset.reset = earliest
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	check.crcs = true
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-offsets
debezium         | 	client.rack = 
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	default.api.timeout.ms = 60000
debezium         | 	enable.auto.commit = false
debezium         | 	enable.metrics.push = true
debezium         | 	exclude.internal.topics = true
debezium         | 	fetch.max.bytes = 52428800
debezium         | 	fetch.max.wait.ms = 500
debezium         | 	fetch.min.bytes = 1
debezium         | 	group.id = 1
debezium         | 	group.instance.id = null
debezium         | 	group.protocol = classic
debezium         | 	group.remote.assignor = null
debezium         | 	heartbeat.interval.ms = 3000
debezium         | 	interceptor.classes = []
debezium         | 	internal.leave.group.on.close = true
debezium         | 	internal.throw.on.fetch.stable.offset.unsupported = false
debezium         | 	isolation.level = read_uncommitted
debezium         | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
debezium         | 	max.partition.fetch.bytes = 1048576
debezium         | 	max.poll.interval.ms = 300000
debezium         | 	max.poll.records = 500
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
debezium         | 	receive.buffer.bytes = 65536
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	session.timeout.ms = 45000
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
debezium         |    [org.apache.kafka.clients.consumer.ConsumerConfig]
debezium         | 2025-01-14 19:31:34,785 INFO   ||  [Producer clientId=1-offsets] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:34,789 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
schema-registry  | [2025-01-14 19:31:34,802] INFO jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.17+8-LTS (org.eclipse.jetty.server.Server)
debezium         | 2025-01-14 19:31:34,805 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
debezium         | 2025-01-14 19:31:34,805 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,805 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,805 INFO   ||  Kafka startTimeMs: 1736883094805   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,810 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:34,814 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Assigned to partition(s): connect_offsets-0, connect_offsets-5, connect_offsets-10, connect_offsets-20, connect_offsets-15, connect_offsets-9, connect_offsets-11, connect_offsets-16, connect_offsets-4, connect_offsets-17, connect_offsets-3, connect_offsets-24, connect_offsets-23, connect_offsets-13, connect_offsets-18, connect_offsets-22, connect_offsets-8, connect_offsets-2, connect_offsets-12, connect_offsets-19, connect_offsets-14, connect_offsets-1, connect_offsets-6, connect_offsets-7, connect_offsets-21   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
debezium         | 2025-01-14 19:31:34,815 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,815 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-5   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,815 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-10   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,815 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-20   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,815 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-15   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,815 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-9   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-11   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-16   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-4   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-17   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-3   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-24   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-23   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-13   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-18   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-22   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-8   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-2   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-12   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-19   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-14   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-1   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-6   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-7   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,816 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Seeking to earliest offset of partition connect_offsets-21   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
schema-registry  | [2025-01-14 19:31:34,831] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
schema-registry  | [2025-01-14 19:31:34,831] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
schema-registry  | [2025-01-14 19:31:34,832] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
debezium         | 2025-01-14 19:31:34,837 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,838 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,839 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting offset for partition connect_offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,839 INFO   ||  Finished reading KafkaBasedLog for topic connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:34,839 INFO   ||  Started KafkaBasedLog for topic connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:34,839 INFO   ||  Finished reading offsets topic and starting KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
debezium         | 2025-01-14 19:31:34,840 INFO   ||  Worker started   [org.apache.kafka.connect.runtime.Worker]
debezium         | 2025-01-14 19:31:34,840 INFO   ||  Starting KafkaBasedLog with topic connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
broker           | [2025-01-14 19:31:34,845] INFO Creating topic connect_statuses with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1] New topics: [Set(connect_statuses)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect_statuses,Some(D3ruEIx0TqOP3x1QNKoJ4Q),HashMap(connect_statuses-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_statuses-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_statuses-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_statuses-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect_statuses-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1] New partition creation callback for connect_statuses-1,connect_statuses-0,connect_statuses-4,connect_statuses-2,connect_statuses-3 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,852] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,853] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,853] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-3 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,853] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-1 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,853] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-2 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,853] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-4 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,853] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-0 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,853] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Changed partition connect_statuses-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,859] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_statuses-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,859] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_statuses-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,859] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_statuses-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,859] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_statuses-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,859] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_statuses-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 5 become-leader and 0 become-follower partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,859] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 5 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-3 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-1 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-2 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-4 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_statuses-0 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,860] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,860] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 for 5 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 7 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 7 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 7 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 7 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,860] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_statuses', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 7 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,861] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect_statuses-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,861] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect_statuses-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,861] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect_statuses-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,861] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect_statuses-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,861] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect_statuses-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,861] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(connect_statuses-1, connect_statuses-0, connect_statuses-4, connect_statuses-2, connect_statuses-3) (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:31:34,861] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 1 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,863] INFO [LogLoader partition=connect_statuses-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,863] INFO Created log for partition connect_statuses-2 in /var/lib/kafka/data/connect_statuses-2 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,864] INFO [Partition connect_statuses-2 broker=1] No checkpointed highwatermark is found for partition connect_statuses-2 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,864] INFO [Partition connect_statuses-2 broker=1] Log loaded for partition connect_statuses-2 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,864] INFO [Broker id=1] Leader connect_statuses-2 with topic id Some(D3ruEIx0TqOP3x1QNKoJ4Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,868] INFO [LogLoader partition=connect_statuses-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,869] INFO Created log for partition connect_statuses-3 in /var/lib/kafka/data/connect_statuses-3 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,869] INFO [Partition connect_statuses-3 broker=1] No checkpointed highwatermark is found for partition connect_statuses-3 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,869] INFO [Partition connect_statuses-3 broker=1] Log loaded for partition connect_statuses-3 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,869] INFO [Broker id=1] Leader connect_statuses-3 with topic id Some(D3ruEIx0TqOP3x1QNKoJ4Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,873] INFO [LogLoader partition=connect_statuses-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,874] INFO Created log for partition connect_statuses-0 in /var/lib/kafka/data/connect_statuses-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,874] INFO [Partition connect_statuses-0 broker=1] No checkpointed highwatermark is found for partition connect_statuses-0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,874] INFO [Partition connect_statuses-0 broker=1] Log loaded for partition connect_statuses-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,874] INFO [Broker id=1] Leader connect_statuses-0 with topic id Some(D3ruEIx0TqOP3x1QNKoJ4Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,879] INFO [LogLoader partition=connect_statuses-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,880] INFO Created log for partition connect_statuses-1 in /var/lib/kafka/data/connect_statuses-1 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,880] INFO [Partition connect_statuses-1 broker=1] No checkpointed highwatermark is found for partition connect_statuses-1 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,880] INFO [Partition connect_statuses-1 broker=1] Log loaded for partition connect_statuses-1 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,880] INFO [Broker id=1] Leader connect_statuses-1 with topic id Some(D3ruEIx0TqOP3x1QNKoJ4Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,885] INFO [LogLoader partition=connect_statuses-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:34,885] INFO Created log for partition connect_statuses-4 in /var/lib/kafka/data/connect_statuses-4 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:34,885] INFO [Partition connect_statuses-4 broker=1] No checkpointed highwatermark is found for partition connect_statuses-4 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,885] INFO [Partition connect_statuses-4 broker=1] Log loaded for partition connect_statuses-4 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:34,885] INFO [Broker id=1] Leader connect_statuses-4 with topic id Some(D3ruEIx0TqOP3x1QNKoJ4Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:34,888] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect_statuses-2 (state.change.logger)
broker           | [2025-01-14 19:31:34,888] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect_statuses-3 (state.change.logger)
broker           | [2025-01-14 19:31:34,888] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect_statuses-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,888] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect_statuses-1 (state.change.logger)
broker           | [2025-01-14 19:31:34,888] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect_statuses-4 (state.change.logger)
broker           | [2025-01-14 19:31:34,889] INFO [Broker id=1] Finished LeaderAndIsr request in 29ms correlationId 7 from controller 1 for 5 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,889] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=D3ruEIx0TqOP3x1QNKoJ4Q, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=4, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=3, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=2, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=1, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 7 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:34,890] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_statuses', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_statuses-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
broker           | [2025-01-14 19:31:34,890] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_statuses', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_statuses-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
broker           | [2025-01-14 19:31:34,890] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_statuses', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_statuses-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
broker           | [2025-01-14 19:31:34,890] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_statuses', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_statuses-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
broker           | [2025-01-14 19:31:34,890] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_statuses', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_statuses-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
broker           | [2025-01-14 19:31:34,890] INFO [Broker id=1] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
broker           | [2025-01-14 19:31:34,891] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 8 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
debezium         | 2025-01-14 19:31:34,891 INFO   ||  Created topic (name=connect_statuses, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092   [org.apache.kafka.connect.util.TopicAdmin]
debezium         | 2025-01-14 19:31:34,892 INFO   ||  ProducerConfig values: 
debezium         | 	acks = -1
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	batch.size = 16384
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	buffer.memory = 33554432
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-statuses
debezium         | 	compression.type = none
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	delivery.timeout.ms = 120000
debezium         | 	enable.idempotence = false
debezium         | 	enable.metrics.push = true
debezium         | 	interceptor.classes = []
debezium         | 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
debezium         | 	linger.ms = 0
debezium         | 	max.block.ms = 60000
debezium         | 	max.in.flight.requests.per.connection = 1
debezium         | 	max.request.size = 1048576
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metadata.max.idle.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	partitioner.adaptive.partitioning.enable = true
debezium         | 	partitioner.availability.timeout.ms = 0
debezium         | 	partitioner.class = null
debezium         | 	partitioner.ignore.keys = false
debezium         | 	receive.buffer.bytes = 32768
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retries = 0
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	transaction.timeout.ms = 60000
debezium         | 	transactional.id = null
debezium         | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
debezium         |    [org.apache.kafka.clients.producer.ProducerConfig]
debezium         | 2025-01-14 19:31:34,892 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
debezium         | 2025-01-14 19:31:34,894 INFO   ||  These configurations '[group.id, rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.producer.ProducerConfig]
debezium         | 2025-01-14 19:31:34,894 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,894 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,894 INFO   ||  Kafka startTimeMs: 1736883094894   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,895 INFO   ||  ConsumerConfig values: 
debezium         | 	allow.auto.create.topics = true
debezium         | 	auto.commit.interval.ms = 5000
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	auto.offset.reset = earliest
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	check.crcs = true
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-statuses
debezium         | 	client.rack = 
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	default.api.timeout.ms = 60000
debezium         | 	enable.auto.commit = false
debezium         | 	enable.metrics.push = true
debezium         | 	exclude.internal.topics = true
debezium         | 	fetch.max.bytes = 52428800
debezium         | 	fetch.max.wait.ms = 500
debezium         | 	fetch.min.bytes = 1
debezium         | 	group.id = 1
debezium         | 	group.instance.id = null
debezium         | 	group.protocol = classic
debezium         | 	group.remote.assignor = null
debezium         | 	heartbeat.interval.ms = 3000
debezium         | 	interceptor.classes = []
debezium         | 	internal.leave.group.on.close = true
debezium         | 	internal.throw.on.fetch.stable.offset.unsupported = false
debezium         | 	isolation.level = read_uncommitted
debezium         | 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
debezium         | 	max.partition.fetch.bytes = 1048576
debezium         | 	max.poll.interval.ms = 300000
debezium         | 	max.poll.records = 500
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
debezium         | 	receive.buffer.bytes = 65536
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	session.timeout.ms = 45000
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
debezium         |    [org.apache.kafka.clients.consumer.ConsumerConfig]
debezium         | 2025-01-14 19:31:34,895 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
debezium         | 2025-01-14 19:31:34,897 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
debezium         | 2025-01-14 19:31:34,897 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,897 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,897 INFO   ||  Kafka startTimeMs: 1736883094897   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:34,897 INFO   ||  [Producer clientId=1-statuses] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:34,901 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:34,902 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Assigned to partition(s): connect_statuses-0, connect_statuses-1, connect_statuses-4, connect_statuses-2, connect_statuses-3   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
debezium         | 2025-01-14 19:31:34,902 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition connect_statuses-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,902 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition connect_statuses-1   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,903 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition connect_statuses-4   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,903 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition connect_statuses-2   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,903 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition connect_statuses-3   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition connect_statuses-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition connect_statuses-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition connect_statuses-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition connect_statuses-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition connect_statuses-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  Finished reading KafkaBasedLog for topic connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:34,910 INFO   ||  Started KafkaBasedLog for topic connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:34,912 INFO   ||  Starting KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
debezium         | 2025-01-14 19:31:34,912 INFO   ||  Starting KafkaBasedLog with topic connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
broker           | [2025-01-14 19:31:34,916] INFO Creating topic connect_configs with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
broker           | [2025-01-14 19:31:34,921] INFO [Controller id=1] New topics: [HashSet(connect_configs)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(connect_configs,Some(fmXnTlxbQjy95J7vOgONuQ),Map(connect_configs-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,922] INFO [Controller id=1] New partition creation callback for connect_configs-0 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:31:34,922] INFO [Controller id=1 epoch=1] Changed partition connect_configs-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,922] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,922] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_configs-0 from NonExistentReplica to NewReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,922] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,926] INFO [Controller id=1 epoch=1] Changed partition connect_configs-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), leaderRecoveryState=RECOVERED, partitionEpoch=0) (state.change.logger)
broker           | [2025-01-14 19:31:34,926] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_configs', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) to broker 1 for partition connect_configs-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,926] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,926] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,926] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect_configs-0 from NewReplica to OnlineReplica (state.change.logger)
broker           | [2025-01-14 19:31:34,926] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,927] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:34,927] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect_configs', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], partitionEpoch=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true, leaderRecoveryState=0) correlation id 9 from controller 1 epoch 1 (state.change.logger)
broker           | [2025-01-14 19:31:34,927] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition connect_configs-0 (state.change.logger)
broker           | [2025-01-14 19:31:34,927] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect_configs-0) (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:31:34,927] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:35,001] INFO [LogLoader partition=connect_configs-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
broker           | [2025-01-14 19:31:35,001] INFO Created log for partition connect_configs-0 in /var/lib/kafka/data/connect_configs-0 with properties {cleanup.policy=compact} (kafka.log.LogManager)
broker           | [2025-01-14 19:31:35,002] INFO [Partition connect_configs-0 broker=1] No checkpointed highwatermark is found for partition connect_configs-0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:35,002] INFO [Partition connect_configs-0 broker=1] Log loaded for partition connect_configs-0 with initial high watermark 0 (kafka.cluster.Partition)
broker           | [2025-01-14 19:31:35,002] INFO [Broker id=1] Leader connect_configs-0 with topic id Some(fmXnTlxbQjy95J7vOgONuQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas []. Previous leader epoch was -1. (state.change.logger)
broker           | [2025-01-14 19:31:35,006] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition connect_configs-0 (state.change.logger)
broker           | [2025-01-14 19:31:35,006] INFO [Broker id=1] Finished LeaderAndIsr request in 79ms correlationId 9 from controller 1 for 1 partitions (state.change.logger)
broker           | [2025-01-14 19:31:35,007] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=fmXnTlxbQjy95J7vOgONuQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 9 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
broker           | [2025-01-14 19:31:35,008] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect_configs', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect_configs-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
broker           | [2025-01-14 19:31:35,008] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
broker           | [2025-01-14 19:31:35,009] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 10 sent to broker broker:29092 (id: 1 rack: null) (state.change.logger)
debezium         | 2025-01-14 19:31:35,009 INFO   ||  Created topic (name=connect_configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at broker:29092   [org.apache.kafka.connect.util.TopicAdmin]
debezium         | 2025-01-14 19:31:35,010 INFO   ||  ProducerConfig values: 
debezium         | 	acks = -1
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	batch.size = 16384
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	buffer.memory = 33554432
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-configs
debezium         | 	compression.type = none
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	delivery.timeout.ms = 2147483647
debezium         | 	enable.idempotence = false
debezium         | 	enable.metrics.push = true
debezium         | 	interceptor.classes = []
debezium         | 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
debezium         | 	linger.ms = 0
debezium         | 	max.block.ms = 60000
debezium         | 	max.in.flight.requests.per.connection = 1
debezium         | 	max.request.size = 1048576
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metadata.max.idle.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	partitioner.adaptive.partitioning.enable = true
debezium         | 	partitioner.availability.timeout.ms = 0
debezium         | 	partitioner.class = null
debezium         | 	partitioner.ignore.keys = false
debezium         | 	receive.buffer.bytes = 32768
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retries = 2147483647
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	transaction.timeout.ms = 60000
debezium         | 	transactional.id = null
debezium         | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
debezium         |    [org.apache.kafka.clients.producer.ProducerConfig]
debezium         | 2025-01-14 19:31:35,010 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
debezium         | 2025-01-14 19:31:35,014 INFO   ||  These configurations '[group.id, rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.producer.ProducerConfig]
debezium         | 2025-01-14 19:31:35,014 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:35,014 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:35,014 INFO   ||  Kafka startTimeMs: 1736883095014   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:35,015 INFO   ||  ConsumerConfig values: 
debezium         | 	allow.auto.create.topics = true
debezium         | 	auto.commit.interval.ms = 5000
debezium         | 	auto.include.jmx.reporter = true
debezium         | 	auto.offset.reset = earliest
debezium         | 	bootstrap.servers = [broker:29092]
debezium         | 	check.crcs = true
debezium         | 	client.dns.lookup = use_all_dns_ips
debezium         | 	client.id = 1-configs
debezium         | 	client.rack = 
debezium         | 	connections.max.idle.ms = 540000
debezium         | 	default.api.timeout.ms = 60000
debezium         | 	enable.auto.commit = false
debezium         | 	enable.metrics.push = true
debezium         | 	exclude.internal.topics = true
debezium         | 	fetch.max.bytes = 52428800
debezium         | 	fetch.max.wait.ms = 500
debezium         | 	fetch.min.bytes = 1
debezium         | 	group.id = 1
debezium         | 	group.instance.id = null
debezium         | 	group.protocol = classic
debezium         | 	group.remote.assignor = null
debezium         | 	heartbeat.interval.ms = 3000
debezium         | 	interceptor.classes = []
debezium         | 	internal.leave.group.on.close = true
debezium         | 	internal.throw.on.fetch.stable.offset.unsupported = false
debezium         | 	isolation.level = read_uncommitted
debezium         | 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
debezium         | 	max.partition.fetch.bytes = 1048576
debezium         | 	max.poll.interval.ms = 300000
debezium         | 	max.poll.records = 500
debezium         | 	metadata.max.age.ms = 300000
debezium         | 	metric.reporters = []
debezium         | 	metrics.num.samples = 2
debezium         | 	metrics.recording.level = INFO
debezium         | 	metrics.sample.window.ms = 30000
debezium         | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
debezium         | 	receive.buffer.bytes = 65536
debezium         | 	reconnect.backoff.max.ms = 1000
debezium         | 	reconnect.backoff.ms = 50
debezium         | 	request.timeout.ms = 30000
debezium         | 	retry.backoff.max.ms = 1000
debezium         | 	retry.backoff.ms = 100
debezium         | 	sasl.client.callback.handler.class = null
debezium         | 	sasl.jaas.config = null
debezium         | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
debezium         | 	sasl.kerberos.min.time.before.relogin = 60000
debezium         | 	sasl.kerberos.service.name = null
debezium         | 	sasl.kerberos.ticket.renew.jitter = 0.05
debezium         | 	sasl.kerberos.ticket.renew.window.factor = 0.8
debezium         | 	sasl.login.callback.handler.class = null
debezium         | 	sasl.login.class = null
debezium         | 	sasl.login.connect.timeout.ms = null
debezium         | 	sasl.login.read.timeout.ms = null
debezium         | 	sasl.login.refresh.buffer.seconds = 300
debezium         | 	sasl.login.refresh.min.period.seconds = 60
debezium         | 	sasl.login.refresh.window.factor = 0.8
debezium         | 	sasl.login.refresh.window.jitter = 0.05
debezium         | 	sasl.login.retry.backoff.max.ms = 10000
debezium         | 	sasl.login.retry.backoff.ms = 100
debezium         | 	sasl.mechanism = GSSAPI
debezium         | 	sasl.oauthbearer.clock.skew.seconds = 30
debezium         | 	sasl.oauthbearer.expected.audience = null
debezium         | 	sasl.oauthbearer.expected.issuer = null
debezium         | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
debezium         | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
debezium         | 	sasl.oauthbearer.jwks.endpoint.url = null
debezium         | 	sasl.oauthbearer.scope.claim.name = scope
debezium         | 	sasl.oauthbearer.sub.claim.name = sub
debezium         | 	sasl.oauthbearer.token.endpoint.url = null
debezium         | 	security.protocol = PLAINTEXT
debezium         | 	security.providers = null
debezium         | 	send.buffer.bytes = 131072
debezium         | 	session.timeout.ms = 45000
debezium         | 	socket.connection.setup.timeout.max.ms = 30000
debezium         | 	socket.connection.setup.timeout.ms = 10000
debezium         | 	ssl.cipher.suites = null
debezium         | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
debezium         | 	ssl.endpoint.identification.algorithm = https
debezium         | 	ssl.engine.factory.class = null
debezium         | 	ssl.key.password = null
debezium         | 	ssl.keymanager.algorithm = SunX509
debezium         | 	ssl.keystore.certificate.chain = null
debezium         | 	ssl.keystore.key = null
debezium         | 	ssl.keystore.location = null
debezium         | 	ssl.keystore.password = null
debezium         | 	ssl.keystore.type = JKS
debezium         | 	ssl.protocol = TLSv1.3
debezium         | 	ssl.provider = null
debezium         | 	ssl.secure.random.implementation = null
debezium         | 	ssl.trustmanager.algorithm = PKIX
debezium         | 	ssl.truststore.certificates = null
debezium         | 	ssl.truststore.location = null
debezium         | 	ssl.truststore.password = null
debezium         | 	ssl.truststore.type = JKS
debezium         | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
debezium         |    [org.apache.kafka.clients.consumer.ConsumerConfig]
debezium         | 2025-01-14 19:31:35,015 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
debezium         | 2025-01-14 19:31:35,018 INFO   ||  [Producer clientId=1-configs] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:35,019 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
debezium         | 2025-01-14 19:31:35,019 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:35,020 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:35,020 INFO   ||  Kafka startTimeMs: 1736883095019   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:31:35,025 INFO   ||  [Consumer clientId=1-configs, groupId=1] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:35,026 INFO   ||  [Consumer clientId=1-configs, groupId=1] Assigned to partition(s): connect_configs-0   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
debezium         | 2025-01-14 19:31:35,026 INFO   ||  [Consumer clientId=1-configs, groupId=1] Seeking to earliest offset of partition connect_configs-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:35,034 INFO   ||  [Consumer clientId=1-configs, groupId=1] Resetting offset for partition connect_configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[broker:29092 (id: 1 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
debezium         | 2025-01-14 19:31:35,034 INFO   ||  Finished reading KafkaBasedLog for topic connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:35,034 INFO   ||  Started KafkaBasedLog for topic connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:31:35,034 INFO   ||  Started KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
debezium         | 2025-01-14 19:31:35,035 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Herder started   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:31:35,044 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Cluster ID: __R4m4AETriw5M5Mmqky6Q   [org.apache.kafka.clients.Metadata]
debezium         | 2025-01-14 19:31:35,045 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Discovered group coordinator broker:29092 (id: 2147483646 rack: null)   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
debezium         | 2025-01-14 19:31:35,046 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
debezium         | 2025-01-14 19:31:35,046 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
broker           | [2025-01-14 19:31:35,050] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group 1 in Empty state. Created a new member id connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
debezium         | 2025-01-14 19:31:35,051 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
broker           | [2025-01-14 19:31:35,053] INFO [GroupCoordinator 1]: Preparing to rebalance group 1 in state PreparingRebalance with old generation 0 (__consumer_offsets-49) (reason: Adding new member connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a with group instance id None; client reason: ) (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:31:35,054] INFO [GroupCoordinator 1]: Stabilized group 1 generation 1 (__consumer_offsets-49) with 1 members (kafka.coordinator.group.GroupCoordinator)
debezium         | 2025-01-14 19:31:35,055 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Successfully joined group with generation Generation{generationId=1, memberId='connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
broker           | [2025-01-14 19:31:35,061] INFO [GroupCoordinator 1]: Assignment received from leader connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a for group 1 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
debezium         | 2025-01-14 19:31:35,063 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Successfully synced group in generation Generation{generationId=1, memberId='connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
debezium         | 2025-01-14 19:31:35,063 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a', leaderUrl='http://172.18.0.8:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:31:35,064 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Starting connectors and tasks using config offset -1   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:31:35,067 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:31:35,102 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Session key updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
schema-registry  | [2025-01-14 19:31:35,123] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
debezium         | 2025-01-14 19:31:35,269 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:31:35 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 46   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:31:35,287] INFO Started o.e.j.s.ServletContextHandler@64a9d48c{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry  | [2025-01-14 19:31:35,295] INFO Started o.e.j.s.ServletContextHandler@507b79f7{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry  | [2025-01-14 19:31:35,352] INFO Started NetworkTrafficServerConnector@19b93fa8{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8081} (org.eclipse.jetty.server.AbstractConnector)
schema-registry  | [2025-01-14 19:31:35,353] INFO Started @2911ms (org.eclipse.jetty.server.Server)
schema-registry  | [2025-01-14 19:31:35,355] INFO Schema Registry version: 7.3.1 commitId: f5c939433bfbf9ef90a070cce1a778c0b5ea5c53 (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)
schema-registry  | [2025-01-14 19:31:35,356] INFO Server started, listening for requests... (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)
debezium-ui      | exec java -Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -XX:+ExitOnOutOfMemoryError -cp . -jar /deployments/quarkus-run.jar
debezium-ui      | __  ____  __  _____   ___  __ ____  ______ 
debezium-ui      |  --/ __ \/ / / / _ | / _ \/ //_/ / / / __/ 
debezium-ui      |  -/ /_/ / /_/ / __ |/ , _/ ,< / /_/ /\ \   
debezium-ui      | --\___\_\____/_/ |_/_/|_/_/|_|\____/___/   
debezium-ui      | 2025-01-14 19:31:36,408 INFO  [io.quarkus] (main) debezium-ui-backend 2.0.0-SNAPSHOT on JVM (powered by Quarkus 1.12.1.Final) started in 0.389s. Listening on: http://0.0.0.0:8080
debezium-ui      | 2025-01-14 19:31:36,417 INFO  [io.quarkus] (main) Profile prod activated. 
debezium-ui      | 2025-01-14 19:31:36,417 INFO  [io.quarkus] (main) Installed features: [cdi, rest-client, resteasy, resteasy-jsonb, smallrye-openapi]
schema-registry  | [2025-01-14 19:31:40,209] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:31:40 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 110 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:31:43,515] INFO Processing srvr command from /172.18.0.3:58000 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:31:45,300 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:31:45 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 4   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:31:50,246] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:31:50 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 3 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:31:53,560] INFO Processing srvr command from /172.18.0.3:55326 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:31:55,338 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:31:55 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:00,293] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:00 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 4 (io.confluent.rest-utils.requests)
debezium         | 2025-01-14 19:32:01,979 WARN   ||  Ignoring invalid completion time 1736883121979 since it is before this stage's start time of 1736883122380   [org.apache.kafka.connect.util.Stage]
debezium         | 2025-01-14 19:32:01,529 WARN   ||  Ignoring invalid completion time 1736883121529 since it is before this stage's start time of 1736883121980   [org.apache.kafka.connect.util.Stage]
zookeeper        | [2025-01-14 19:32:02,555] INFO Processing srvr command from /172.18.0.3:42616 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:32:04,323 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:04 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:09,282] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:09 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 3 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:32:12,598] INFO Processing srvr command from /172.18.0.3:45728 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:32:14,363 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:14 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:19,327] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:19 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 3 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:32:22,645] INFO Processing srvr command from /172.18.0.3:46312 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:32:24,428 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:24 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:32:28,556 INFO   ||  172.18.0.9 - - [14/Jan/2025:19:32:28 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "Apache-HttpClient/4.5.13 (Java/11.0.21)" 1   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:29,370] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:29 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 3 (io.confluent.rest-utils.requests)
debezium         | 2025-01-14 19:32:30,903 WARN   ||  Ignoring invalid completion time 1736883150903 since it is before this stage's start time of 1736883151322   [org.apache.kafka.connect.util.Stage]
debezium         | 2025-01-14 19:32:30,451 WARN   ||  Ignoring invalid completion time 1736883150451 since it is before this stage's start time of 1736883150903   [org.apache.kafka.connect.util.Stage]
zookeeper        | [2025-01-14 19:32:31,618] INFO Processing srvr command from /172.18.0.3:42758 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:32:33,395 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:33 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:38,331] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:38 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 2 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:32:41,662] INFO Processing srvr command from /172.18.0.3:35746 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:32:43,438 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:43 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:48,374] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:48 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 2 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:32:51,691] INFO Processing srvr command from /172.18.0.3:60900 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:32:53,470 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:53 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 1   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:32:58,405] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:32:58 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 2 (io.confluent.rest-utils.requests)
debezium         | 2025-01-14 19:32:59,822 WARN   ||  Ignoring invalid completion time 1736883179822 since it is before this stage's start time of 1736883180226   [org.apache.kafka.connect.util.Stage]
debezium         | 2025-01-14 19:32:59,369 WARN   ||  Ignoring invalid completion time 1736883179369 since it is before this stage's start time of 1736883179822   [org.apache.kafka.connect.util.Stage]
zookeeper        | [2025-01-14 19:33:00,678] INFO Processing srvr command from /172.18.0.3:52420 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:33:02,466 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:02 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:33:07,392] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:07 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 3 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:33:10,723] INFO Processing srvr command from /172.18.0.3:57816 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:33:12,504 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:12 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:33:17,428] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:17 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 2 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:33:20,758] INFO Processing srvr command from /172.18.0.3:40790 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:33:22,541 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:22 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:33:27,476] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:27 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 3 (io.confluent.rest-utils.requests)
debezium         | 2025-01-14 19:33:28,361 INFO   ||  172.18.0.9 - - [14/Jan/2025:19:33:28 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "Apache-HttpClient/4.5.13 (Java/11.0.21)" 1   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:33:28,270 WARN   ||  Ignoring invalid completion time 1736883208270 since it is before this stage's start time of 1736883209226   [org.apache.kafka.connect.util.Stage]
zookeeper        | [2025-01-14 19:33:29,748] INFO Processing srvr command from /172.18.0.3:40630 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:33:31,546 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:31 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 2   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:33:36,458] INFO [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:36 +0000] "GET /subjects HTTP/1.1" 200 2 "-" "curl/7.61.1" 2 (io.confluent.rest-utils.requests)
zookeeper        | [2025-01-14 19:33:39,819] INFO Processing srvr command from /172.18.0.3:36430 (org.apache.zookeeper.server.NIOServerCnxn)
debezium         | 2025-01-14 19:33:41,572 INFO   ||  [0:0:0:0:0:0:0:1] - - [14/Jan/2025:19:33:41 +0000] "GET /connectors HTTP/1.1" 200 2 "-" "curl/8.6.0" 1   [org.apache.kafka.connect.runtime.rest.RestServer]
schema-registry  | [2025-01-14 19:33:43,609] INFO Stopped NetworkTrafficServerConnector@19b93fa8{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8081} (org.eclipse.jetty.server.AbstractConnector)
schema-registry  | [2025-01-14 19:33:43,609] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
schema-registry  | [2025-01-14 19:33:43,610] INFO Stopped o.e.j.s.ServletContextHandler@507b79f7{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
rest-proxy       | [2025-01-14 19:33:43,624] INFO Stopped NetworkTrafficServerConnector@6c2ed0cd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8082} (org.eclipse.jetty.server.AbstractConnector)
rest-proxy       | [2025-01-14 19:33:43,626] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
rest-proxy       | [2025-01-14 19:33:43,629] INFO Stopped o.e.j.s.ServletContextHandler@5f574cc2{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry  | [2025-01-14 19:33:43,629] INFO Stopped o.e.j.s.ServletContextHandler@64a9d48c{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
schema-registry  | [2025-01-14 19:33:43,632] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,632] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,632] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,633] INFO Shutting down schema registry (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
schema-registry  | [2025-01-14 19:33:43,633] INFO [kafka-store-reader-thread-_schemas]: Shutting down (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:33:43,634] INFO [kafka-store-reader-thread-_schemas]: Shutdown completed (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:33:43,634] INFO [kafka-store-reader-thread-_schemas]: Stopped (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:33:43,634] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
schema-registry  | [2025-01-14 19:33:43,634] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
schema-registry  | [2025-01-14 19:33:43,635] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,635] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,635] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,637] INFO App info kafka.consumer for KafkaStore-reader-_schemas unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:33:43,637] INFO KafkaStoreReaderThread shutdown complete. (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
schema-registry  | [2025-01-14 19:33:43,637] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
schema-registry  | [2025-01-14 19:33:43,639] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,639] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,639] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,639] INFO App info kafka.producer for producer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
schema-registry  | [2025-01-14 19:33:43,639] INFO Kafka store producer shut down (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:33:43,639] INFO Kafka store shut down complete (io.confluent.kafka.schemaregistry.storage.KafkaStore)
schema-registry  | [2025-01-14 19:33:43,641] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Member sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117 sending LeaveGroup request to coordinator broker:29092 (id: 2147483646 rack: null) due to the consumer is being closed (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:33:43,642] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Resetting generation and member id due to: consumer pro-actively leaving the group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:33:43,642] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Request joining group due to: consumer pro-actively leaving the group (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:33:43,642] WARN [Schema registry clientId=sr-1, groupId=schema-registry] Close timed out with 1 pending requests to coordinator, terminating client connections (io.confluent.kafka.schemaregistry.leaderelector.kafka.SchemaRegistryCoordinator)
schema-registry  | [2025-01-14 19:33:43,642] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,642] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,642] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
schema-registry  | [2025-01-14 19:33:43,643] INFO App info kafka.schema.registry for sr-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2025-01-14 19:33:43,647] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 1 (__consumer_offsets-29) (reason: Removing member sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117 on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:33:43,647] INFO [GroupCoordinator 1]: Group schema-registry with generation 2 is now empty (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:33:43,649] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=sr-1-13a5cfad-6f92-4cb9-9a71-65e6c4a1e117, groupInstanceId=None, clientId=sr-1, clientHost=/172.18.0.6, sessionTimeoutMs=10000, rebalanceTimeoutMs=300000, supportedProtocols=List(v0)) has left group schema-registry through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
rest-proxy       | [2025-01-14 19:33:43,652] INFO Stopped o.e.j.s.ServletContextHandler@680bddf5{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
debezium-ui      | 2025-01-14 19:33:43,652 INFO  [io.quarkus] (Shutdown thread) debezium-ui-backend stopped in 0.050s
rest-proxy       | [2025-01-14 19:33:43,655] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
rest-proxy       | [2025-01-14 19:33:43,655] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
rest-proxy       | [2025-01-14 19:33:43,655] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
[Kpostgres_slave exited with code 0
[Kdebezium-ui exited with code 143
debezium         | 2025-01-14 19:33:44,274 INFO   ||  Kafka Connect stopping   [org.apache.kafka.connect.runtime.Connect]
debezium         | 2025-01-14 19:33:44,274 INFO   ||  Stopping REST server   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:33:44,279 INFO   ||  Stopped http_8083@5f25b34d{HTTP/1.1, (http/1.1)}{0.0.0.0:8083}   [org.eclipse.jetty.server.AbstractConnector]
debezium         | 2025-01-14 19:33:44,279 INFO   ||  node0 Stopped scavenging   [org.eclipse.jetty.server.session]
debezium         | 2025-01-14 19:33:44,281 INFO   ||  Stopped o.e.j.s.ServletContextHandler@93bc31f{/,null,STOPPED}   [org.eclipse.jetty.server.handler.ContextHandler]
debezium         | 2025-01-14 19:33:44,281 INFO   ||  REST server stopped   [org.apache.kafka.connect.runtime.rest.RestServer]
debezium         | 2025-01-14 19:33:44,281 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Herder stopping   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:33:44,281 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Stopping connectors and tasks that are still assigned to this worker.   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:33:44,281 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Member connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a sending LeaveGroup request to coordinator broker:29092 (id: 2147483646 rack: null) due to the consumer is being closed   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
debezium         | 2025-01-14 19:33:44,282 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
debezium         | 2025-01-14 19:33:44,282 WARN   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Close timed out with 1 pending requests to coordinator, terminating client connections   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
debezium         | 2025-01-14 19:33:44,282 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,282 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,282 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
broker           | [2025-01-14 19:33:44,282] INFO [GroupCoordinator 1]: Preparing to rebalance group 1 in state PreparingRebalance with old generation 1 (__consumer_offsets-49) (reason: Removing member connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a on LeaveGroup; client reason: the consumer is being closed) (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:33:44,282] INFO [GroupCoordinator 1]: Group 1 with generation 2 is now empty (__consumer_offsets-49) (kafka.coordinator.group.GroupCoordinator)
debezium         | 2025-01-14 19:33:44,283 INFO   ||  App info kafka.connect for connect-172.18.0.8:8083 unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:44,283 INFO   ||  Stopping KafkaBasedLog for topic connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:33:44,284 INFO   ||  [Producer clientId=1-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
broker           | [2025-01-14 19:33:44,285] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=connect-172.18.0.8:8083-be4e1ce1-753d-449b-978d-ea6ac7c51d6a, groupInstanceId=None, clientId=connect-172.18.0.8:8083, clientHost=/172.18.0.8, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(sessioned, compatible, default)) has left group 1 through explicit `LeaveGroup`; client reason: the consumer is being closed (kafka.coordinator.group.GroupCoordinator)
debezium         | 2025-01-14 19:33:44,286 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,286 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,286 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,286 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,287 INFO   ||  App info kafka.producer for 1-statuses unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:44,287 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
debezium         | 2025-01-14 19:33:44,287 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
[Krest-proxy exited with code 143
[Kschema-registry exited with code 143
debezium         | 2025-01-14 19:33:44,620 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,620 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,620 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,620 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,622 INFO   ||  App info kafka.consumer for 1-statuses unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:44,622 INFO   ||  Stopped KafkaBasedLog for topic connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:33:44,622 INFO   ||  Closing KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
debezium         | 2025-01-14 19:33:44,622 INFO   ||  Stopping KafkaBasedLog for topic connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:33:44,622 INFO   ||  [Producer clientId=1-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
debezium         | 2025-01-14 19:33:44,623 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,623 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,623 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,623 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,623 INFO   ||  App info kafka.producer for 1-configs unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:44,623 INFO   ||  [Consumer clientId=1-configs, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
debezium         | 2025-01-14 19:33:44,624 INFO   ||  [Consumer clientId=1-configs, groupId=1] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  App info kafka.consumer for 1-configs unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Stopped KafkaBasedLog for topic connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Closed KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
debezium         | 2025-01-14 19:33:44,839 INFO   ||  Worker stopping   [org.apache.kafka.connect.runtime.Worker]
debezium         | 2025-01-14 19:33:44,840 INFO   ||  Stopping KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
debezium         | 2025-01-14 19:33:44,840 INFO   ||  Stopping KafkaBasedLog for topic connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:33:44,840 INFO   ||  [Producer clientId=1-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  App info kafka.producer for 1-offsets unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
debezium         | 2025-01-14 19:33:44,842 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
debezium         | 2025-01-14 19:33:45,119 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,120 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,120 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,120 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  App info kafka.consumer for 1-offsets unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  Stopped KafkaBasedLog for topic connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  Stopped KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  App info kafka.connect for 172.18.0.8:8083 unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:45,121 INFO   ||  Worker stopped   [org.apache.kafka.connect.runtime.Worker]
debezium         | 2025-01-14 19:33:45,125 INFO   ||  App info kafka.admin.client for 1-shared-admin unregistered   [org.apache.kafka.common.utils.AppInfoParser]
debezium         | 2025-01-14 19:33:45,126 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,126 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,126 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
debezium         | 2025-01-14 19:33:45,126 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Herder stopped   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:33:45,127 INFO   ||  [Worker clientId=connect-172.18.0.8:8083, groupId=1] Herder stopped   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
debezium         | 2025-01-14 19:33:45,127 INFO   ||  Kafka Connect stopped   [org.apache.kafka.connect.runtime.Connect]
[Kdebezium exited with code 143
broker           | [2025-01-14 19:33:45,807] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
broker           | [2025-01-14 19:33:45,808] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
broker           | [2025-01-14 19:33:45,809] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer)
broker           | [2025-01-14 19:33:45,816] INFO [Controller id=1] Shutting down broker 1 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:45,817] DEBUG [Controller id=1] All shutting down brokers: 1 (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:45,817] DEBUG [Controller id=1] Live brokers:  (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:45,818] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
broker           | [2025-01-14 19:33:45,819] TRACE [Controller id=1] All leaders = connect_statuses-3 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-13 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-46 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-9 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-42 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-3 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-21 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-7 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-17 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-11 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-16 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-30 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-20 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-26 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-5 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-24 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-38 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-1 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-34 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-16 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),_schemas-0 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-45 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-0 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-12 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-41 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-4 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_statuses-2 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-24 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-8 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-20 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-49 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-12 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-0 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-13 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-29 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-17 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-25 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-8 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-21 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-37 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-4 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-33 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-15 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-48 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-11 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-44 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-1 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_statuses-1 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-23 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-5 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-19 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-9 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-14 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-32 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-18 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-28 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_configs-0 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-7 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-22 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-40 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-3 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-36 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-47 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_statuses-4 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-14 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-43 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-2 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_statuses-0 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-10 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-6 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-22 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-10 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-18 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-31 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-15 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-27 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-19 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-39 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-6 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),connect_offsets-23 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-35 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1),__consumer_offsets-2 -> (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:0,ZkVersion:0,ControllerEpoch:1) (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:45,821] INFO [KafkaServer id=1] Controlled shutdown request returned successfully after 5ms (kafka.server.KafkaServer)
broker           | [2025-01-14 19:33:45,822] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
broker           | [2025-01-14 19:33:45,823] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
broker           | [2025-01-14 19:33:45,823] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
broker           | [2025-01-14 19:33:45,824] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
broker           | [2025-01-14 19:33:45,830] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
broker           | [2025-01-14 19:33:45,831] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
broker           | [2025-01-14 19:33:45,833] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
broker           | [2025-01-14 19:33:45,836] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,837] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,837] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,837] INFO [KafkaApi-1] Shutdown complete. (kafka.server.KafkaApis)
broker           | [2025-01-14 19:33:45,837] INFO [ExpirationReaper-1-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,838] INFO [ExpirationReaper-1-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,838] INFO [ExpirationReaper-1-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,840] INFO [TransactionCoordinator id=1] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
broker           | [2025-01-14 19:33:45,841] INFO [Transaction State Manager 1]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
broker           | [2025-01-14 19:33:45,842] INFO [Transaction Marker Channel Manager 1]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker           | [2025-01-14 19:33:45,842] INFO [Transaction Marker Channel Manager 1]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker           | [2025-01-14 19:33:45,842] INFO [Transaction Marker Channel Manager 1]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
broker           | [2025-01-14 19:33:45,844] INFO [TransactionCoordinator id=1] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
broker           | [2025-01-14 19:33:45,844] INFO [GroupCoordinator 1]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:33:45,845] INFO [ExpirationReaper-1-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,845] INFO [ExpirationReaper-1-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,845] INFO [ExpirationReaper-1-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,846] INFO [ExpirationReaper-1-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,847] INFO [ExpirationReaper-1-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,847] INFO [ExpirationReaper-1-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,847] INFO [GroupCoordinator 1]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
broker           | [2025-01-14 19:33:45,847] INFO [ReplicaManager broker=1] Shutting down (kafka.server.ReplicaManager)
broker           | [2025-01-14 19:33:45,847] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
broker           | [2025-01-14 19:33:45,848] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
broker           | [2025-01-14 19:33:45,847] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
broker           | [2025-01-14 19:33:45,848] INFO [ReplicaFetcherManager on broker 1] shutting down (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:33:45,849] INFO [ReplicaFetcherManager on broker 1] shutdown completed (kafka.server.ReplicaFetcherManager)
broker           | [2025-01-14 19:33:45,849] INFO [ReplicaAlterLogDirsManager on broker 1] shutting down (kafka.server.ReplicaAlterLogDirsManager)
broker           | [2025-01-14 19:33:45,849] INFO [ReplicaAlterLogDirsManager on broker 1] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
broker           | [2025-01-14 19:33:45,849] INFO [ExpirationReaper-1-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,850] INFO [ExpirationReaper-1-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,850] INFO [ExpirationReaper-1-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,850] INFO [ExpirationReaper-1-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,850] INFO [ExpirationReaper-1-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,850] INFO [ExpirationReaper-1-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,851] INFO [ExpirationReaper-1-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,852] INFO [ExpirationReaper-1-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,852] INFO [ExpirationReaper-1-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,852] INFO [ExpirationReaper-1-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,853] INFO [ExpirationReaper-1-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,853] INFO [ExpirationReaper-1-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
broker           | [2025-01-14 19:33:45,857] INFO [ReplicaManager broker=1] Shut down completely (kafka.server.ReplicaManager)
broker           | [2025-01-14 19:33:45,857] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Shutting down (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:33:45,857] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Stopped (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:33:45,857] INFO [BrokerToControllerChannelManager broker=1 name=alterPartition]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:33:45,858] INFO Broker to controller channel manager for alterPartition shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
broker           | [2025-01-14 19:33:45,859] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:33:45,859] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:33:45,859] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
broker           | [2025-01-14 19:33:45,859] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
broker           | [2025-01-14 19:33:45,860] INFO Shutting down. (kafka.log.LogManager)
broker           | [2025-01-14 19:33:45,860] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
broker           | [2025-01-14 19:33:45,860] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner)
broker           | [2025-01-14 19:33:45,860] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)
broker           | [2025-01-14 19:33:45,860] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner)
broker           | [2025-01-14 19:33:45,888] INFO [ProducerStateManager partition=__consumer_offsets-49] Wrote producer snapshot at offset 2 with 0 producer ids in 2 ms. (kafka.log.ProducerStateManager)
broker           | [2025-01-14 19:33:45,897] INFO [ProducerStateManager partition=connect_configs-0] Wrote producer snapshot at offset 1 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
broker           | [2025-01-14 19:33:45,940] INFO [ProducerStateManager partition=__consumer_offsets-29] Wrote producer snapshot at offset 2 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
broker           | [2025-01-14 19:33:45,995] INFO [ProducerStateManager partition=_schemas-0] Wrote producer snapshot at offset 2 with 0 producer ids in 1 ms. (kafka.log.ProducerStateManager)
broker           | [2025-01-14 19:33:46,022] INFO Shutdown complete. (kafka.log.LogManager)
broker           | [2025-01-14 19:33:46,023] INFO [ControllerEventThread controllerId=1] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread)
broker           | [2025-01-14 19:33:46,023] INFO [ControllerEventThread controllerId=1] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread)
broker           | [2025-01-14 19:33:46,023] INFO [ControllerEventThread controllerId=1] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread)
broker           | [2025-01-14 19:33:46,023] DEBUG [Controller id=1] Resigning (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:46,023] DEBUG [Controller id=1] Unregister BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:46,024] INFO [PartitionStateMachine controllerId=1] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine)
broker           | [2025-01-14 19:33:46,024] INFO [ReplicaStateMachine controllerId=1] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine)
broker           | [2025-01-14 19:33:46,025] INFO [RequestSendThread controllerId=1] Shutting down (kafka.controller.RequestSendThread)
broker           | [2025-01-14 19:33:46,025] INFO [RequestSendThread controllerId=1] Shutdown completed (kafka.controller.RequestSendThread)
broker           | [2025-01-14 19:33:46,025] INFO [RequestSendThread controllerId=1] Stopped (kafka.controller.RequestSendThread)
broker           | [2025-01-14 19:33:46,026] INFO [Controller id=1] Resigned (kafka.controller.KafkaController)
broker           | [2025-01-14 19:33:46,026] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
broker           | [2025-01-14 19:33:46,026] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
broker           | [2025-01-14 19:33:46,026] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
broker           | [2025-01-14 19:33:46,027] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
broker           | [2025-01-14 19:33:46,133] INFO Session: 0x1000826c84a0001 closed (org.apache.zookeeper.ZooKeeper)
broker           | [2025-01-14 19:33:46,133] INFO EventThread shut down for session: 0x1000826c84a0001 (org.apache.zookeeper.ClientCnxn)
broker           | [2025-01-14 19:33:46,134] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
broker           | [2025-01-14 19:33:46,134] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,135] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,135] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,135] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,135] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,135] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,135] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,136] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,136] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,136] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,136] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,136] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
broker           | [2025-01-14 19:33:46,136] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
broker           | [2025-01-14 19:33:46,145] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
broker           | [2025-01-14 19:33:46,145] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
broker           | [2025-01-14 19:33:46,146] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
broker           | [2025-01-14 19:33:46,146] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
broker           | [2025-01-14 19:33:46,146] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
broker           | [2025-01-14 19:33:46,146] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
broker           | [2025-01-14 19:33:46,147] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
[Kpostgres_master exited with code 0
[Kbroker exited with code 143
[Kzookeeper exited with code 143
